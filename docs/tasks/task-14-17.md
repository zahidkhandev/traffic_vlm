# Traffic Scene VLM - ULTRA-DETAILED Tasks 14-17

---

## **PHASE 4-5: MULTIMODAL FUSION & COMPLETE VLM (Day 4-5)**

---

### **Task 14: projection_layer.py - Vision-to-Language Projection**

#### **14.1 Projection Layer Purpose**

**14.1.1 Why Projection is Needed**

```
Problem: Dimension Mismatch

Vision Encoder Output:
  Shape: [B, 196, 768]
  - 196 patches
  - 768-dimensional embeddings

Language Decoder Input:
  Expected: [B, seq_len, 512]
  - 512-dimensional embeddings

Mismatch: 768 ≠ 512

Solution: Projection layer
  - Maps 768 → 512
  - Aligns vision features with language space
  - Learnable transformation

Why not just make them same size?
  - Vision needs more capacity (rich visual features)
  - Language can be smaller (limited vocabulary domain)
  - Projection allows flexibility and optimization
```

**14.1.2 Projection Design Choices**

```
Option 1: Single Linear Layer
  Pros: Simple, fast, few parameters
  Cons: Limited expressiveness

  projection = nn.Linear(768, 512)

Option 2: MLP Projection (Recommended)
  Pros: More expressive, better alignment
  Cons: More parameters, slightly slower

  projection = nn.Sequential(
    nn.Linear(768, 1024),  # Expand
    nn.GELU(),
    nn.Linear(1024, 512)   # Compress
  )

Option 3: Cross-Modal Attention Pooling
  Pros: Learnable pooling, adaptive
  Cons: Complex, many parameters

  Use attention to compress patches

For this project: Use Option 2 (MLP with hidden layer)
  - Good balance of performance and simplicity
  - Standard in vision-language models
```

#### **14.2 MLP Projection Implementation**

**14.2.1 ProjectionLayer Class**

```
Class: VisionLanguageProjection(nn.Module)

Purpose: Project vision features to language embedding space

Attributes:
  - vision_hidden_size: int (768)
  - language_hidden_size: int (512)
  - intermediate_size: int (1024)
  - projection: nn.Sequential

Methods:
  - __init__(config)
  - forward(vision_features) → projected_features

Implementation:
  class VisionLanguageProjection(nn.Module):
    def __init__(self, config):
      super().__init__()

      self.vision_hidden_size = config.vision_hidden_size  # 768
      self.language_hidden_size = config.language_hidden_size  # 512
      self.intermediate_size = config.projection_intermediate_size  # 1024

      # Two-layer MLP with GELU activation
      self.projection = nn.Sequential(
        nn.Linear(self.vision_hidden_size, self.intermediate_size),
        nn.GELU(),
        nn.Linear(self.intermediate_size, self.language_hidden_size)
      )

      # Initialize weights
      self._init_weights()

    def _init_weights(self):
      """Initialize projection weights."""
      for module in self.projection.modules():
        if isinstance(module, nn.Linear):
          nn.init.normal_(module.weight, mean=0.0, std=0.02)
          if module.bias is not None:
            nn.init.zeros_(module.bias)

    def forward(self, vision_features):
      """
      Project vision features to language space.

      Args:
        vision_features: [B, num_patches, vision_hidden_size]
                        e.g., [4, 196, 768]

      Returns:
        projected: [B, num_patches, language_hidden_size]
                  e.g., [4, 196, 512]
      """
      # Apply projection to each patch independently
      # Linear layer operates on last dimension
      projected = self.projection(vision_features)

      # Shape: [B, 196, 512]
      return projected

Parameter Count:
  Layer 1: 768 × 1024 + 1024 = 787,456 params
  Layer 2: 1024 × 512 + 512 = 524,800 params
  Total: 1,312,256 params (~1.3M)

  Memory: 1.3M × 4 bytes = 5.2 MB (FP32)
```

**14.2.2 Forward Pass Example**

```
Example with actual tensors:

Input:
  vision_features = torch.randn(2, 196, 768)
  # Batch of 2 images, 196 patches each, 768-dim features

Step 1: First Linear + GELU
  hidden = nn.Linear(768, 1024)(vision_features)
  # Shape: [2, 196, 1024]

  hidden = GELU(hidden)
  # GELU smoothly gates features
  # Shape: [2, 196, 1024]

Step 2: Second Linear
  output = nn.Linear(1024, 512)(hidden)
  # Shape: [2, 196, 512]

Output:
  projected_features = [2, 196, 512]
  # Now compatible with language decoder!

Key Properties:
  - Each patch processed independently
  - No interaction between patches (that's for attention later)
  - Learned mapping from vision to language space
```

#### **14.3 Alternative Projection Designs**

**14.3.1 Simple Linear Projection**

```
Simplest approach:

class SimpleProjection(nn.Module):
  def __init__(self, config):
    super().__init__()
    self.projection = nn.Linear(
      config.vision_hidden_size,
      config.language_hidden_size
    )

  def forward(self, vision_features):
    return self.projection(vision_features)

When to use:
  - Quick prototyping
  - Limited compute budget
  - When MLP doesn't improve results

For this project: Start with MLP, can simplify if needed
```

**14.3.2 Perceiver-Style Projection**

```
More advanced: Cross-attention based projection

Idea: Use learnable query tokens to pool vision features

class PerceiverProjection(nn.Module):
  def __init__(self, config):
    super().__init__()

    # Learnable query tokens (fewer than input patches)
    self.num_queries = config.num_projection_queries  # e.g., 64
    self.queries = nn.Parameter(
      torch.randn(1, self.num_queries, config.language_hidden_size)
    )

    # Cross-attention: queries attend to vision patches
    self.cross_attention = CrossAttention(config)

  def forward(self, vision_features):
    # vision_features: [B, 196, 768]
    batch_size = vision_features.shape[0]

    # Expand queries for batch
    queries = self.queries.expand(batch_size, -1, -1)  # [B, 64, 512]

    # Cross-attention: queries attend to vision
    output = self.cross_attention(
      query=queries,
      key_value=vision_features
    )
    # Output: [B, 64, 512]

    return output

Benefits:
  - Reduces number of tokens (196 → 64)
  - Learnable compression
  - Better for efficiency

Drawbacks:
  - More complex
  - Potential information loss
  - Harder to train

For this project: Not needed (196 patches is manageable)
```

**14.3.3 Convolutional Projection**

```
Use 1x1 convolution (equivalent to linear per-patch):

class ConvProjection(nn.Module):
  def __init__(self, config):
    super().__init__()
    self.projection = nn.Conv1d(
      config.vision_hidden_size,
      config.language_hidden_size,
      kernel_size=1
    )

  def forward(self, vision_features):
    # Transpose for conv1d: [B, 196, 768] → [B, 768, 196]
    x = vision_features.transpose(1, 2)

    # Apply conv
    x = self.projection(x)  # [B, 512, 196]

    # Transpose back
    x = x.transpose(1, 2)  # [B, 196, 512]

    return x

Equivalent to Linear layer, just different implementation
```

#### **14.4 Projection with Normalization**

**14.4.1 Adding Layer Normalization**

```
For more stable training:

class ProjectionWithNorm(nn.Module):
  def __init__(self, config):
    super().__init__()

    # Pre-projection norm (normalize vision features)
    self.pre_norm = nn.LayerNorm(config.vision_hidden_size)

    # Projection MLP
    self.projection = nn.Sequential(
      nn.Linear(config.vision_hidden_size, config.projection_intermediate_size),
      nn.GELU(),
      nn.Linear(config.projection_intermediate_size, config.language_hidden_size)
    )

    # Post-projection norm (normalize output)
    self.post_norm = nn.LayerNorm(config.language_hidden_size)

  def forward(self, vision_features):
    # Normalize input
    x = self.pre_norm(vision_features)

    # Project
    x = self.projection(x)

    # Normalize output
    x = self.post_norm(x)

    return x

When to add normalization:
  - If training is unstable
  - If vision and language have very different scales
  - Generally helps with deeper models

For this project: Try without first, add if needed
```

**14.4.2 Residual Connection (Optional)**

```
If vision_dim == language_dim, can add residual:

class ResidualProjection(nn.Module):
  def __init__(self, config):
    super().__init__()

    assert config.vision_hidden_size == config.language_hidden_size

    self.projection = nn.Sequential(
      nn.Linear(config.vision_hidden_size, config.projection_intermediate_size),
      nn.GELU(),
      nn.Linear(config.projection_intermediate_size, config.language_hidden_size)
    )

  def forward(self, vision_features):
    return vision_features + self.projection(vision_features)

Not applicable here (768 ≠ 512)
```

#### **14.5 Training Considerations**

**14.5.1 Gradients and Backpropagation**

```
Projection layer is critical bridge between modalities:

Vision Encoder → Projection → Language Decoder
       ↑             ↑              ↑
    grad_vision  grad_proj    grad_language

Gradients flow through projection in both directions:
  - Forward: Transform vision features
  - Backward: Propagate language gradients to vision

If projection is bad:
  - Vision encoder won't learn useful features
  - Language decoder gets poor visual input
  - Model fails to align modalities

Importance: Projection must be well-initialized and trained
```

**14.5.2 Initialization Strategy**

```
Good initialization is crucial:

Why std=0.02?
  - Small enough: Don't disrupt pre-trained vision features
  - Large enough: Allow learning to happen
  - Standard in transformers (BERT, GPT)

Alternative: Xavier/Glorot initialization
  std = sqrt(2 / (in_features + out_features))

  For 768 → 1024: std = sqrt(2 / 1792) ≈ 0.033
  For 1024 → 512: std = sqrt(2 / 1536) ≈ 0.036

Can experiment with both, but 0.02 is safe default
```

**14.5.3 Learning Rate Scaling**

```
Projection might need different learning rate:

Option 1: Same LR as rest of model
  optimizer = AdamW(model.parameters(), lr=1e-4)

Option 2: Higher LR for projection (learns faster)
  optimizer = AdamW([
    {"params": model.vision_encoder.parameters(), "lr": 1e-5},
    {"params": model.projection.parameters(), "lr": 5e-4},  # 5x higher
    {"params": model.decoder.parameters(), "lr": 1e-4}
  ])

Why higher LR?
  - Projection starts from random weights
  - Vision/decoder might be pretrained (need lower LR)
  - Projection needs to learn alignment quickly

For this project: Start with same LR, adjust if needed
```

#### **14.6 Testing & Validation**

**14.6.1 Projection Tests**

```
1. test_projection_shape():
   config = VLMConfig(vision_hidden_size=768, language_hidden_size=512)
   projection = VisionLanguageProjection(config)

   vision_features = torch.randn(4, 196, 768)
   projected = projection(vision_features)

   assert projected.shape == (4, 196, 512)

2. test_projection_gradients():
   projection = VisionLanguageProjection(config)
   vision_features = torch.randn(2, 196, 768, requires_grad=True)

   projected = projection(vision_features)
   loss = projected.sum()
   loss.backward()

   # Check gradients exist
   assert vision_features.grad is not None
   assert not torch.isnan(vision_features.grad).any()

3. test_projection_preserves_batch():
   # Each sample in batch should be independent
   vision_features = torch.randn(3, 196, 768)
   projected = projection(vision_features)

   # Project individually
   proj0 = projection(vision_features[0:1])
   proj1 = projection(vision_features[1:2])
   proj2 = projection(vision_features[2:3])

   assert torch.allclose(projected[0], proj0[0])
   assert torch.allclose(projected[1], proj1[0])
   assert torch.allclose(projected[2], proj2[0])

4. test_projection_not_identity():
   # Projection should transform features
   vision_features = torch.randn(1, 196, 768)
   projected = projection(vision_features)

   # Should not be all zeros
   assert projected.abs().max() > 0.01

   # Should have reasonable scale
   assert projected.abs().mean() < 10.0

5. test_projection_different_batch_sizes():
   for batch_size in [1, 2, 4, 8, 16]:
     vision_features = torch.randn(batch_size, 196, 768)
     projected = projection(vision_features)
     assert projected.shape == (batch_size, 196, 512)
```

**14.6.2 Feature Alignment Visualization**

```
After training, visualize alignment:

def visualize_projection_alignment():
  """
  Check if projected vision features align with language embeddings.
  """
  import matplotlib.pyplot as plt
  from sklearn.decomposition import PCA

  # Get vision features
  vision_features = model.vision_encoder(sample_images)  # [B, 196, 768]
  projected = model.projection(vision_features)  # [B, 196, 512]

  # Get language embeddings
  text_embeddings = model.decoder.embed_tokens.weight  # [vocab_size, 512]

  # Flatten vision features
  projected_flat = projected.reshape(-1, 512)  # [B*196, 512]

  # Combine and reduce to 2D
  all_features = torch.cat([projected_flat, text_embeddings], dim=0)
  pca = PCA(n_components=2)
  features_2d = pca.fit_transform(all_features.detach().cpu().numpy())

  # Split back
  n_vision = projected_flat.shape[0]
  vision_2d = features_2d[:n_vision]
  text_2d = features_2d[n_vision:]

  # Plot
  plt.figure(figsize=(10, 8))
  plt.scatter(vision_2d[:, 0], vision_2d[:, 1],
              alpha=0.3, label='Projected Vision', s=10)
  plt.scatter(text_2d[:, 0], text_2d[:, 1],
              alpha=0.5, label='Text Embeddings', s=30)
  plt.legend()
  plt.title('Vision-Language Alignment (PCA)')
  plt.xlabel('PC1')
  plt.ylabel('PC2')
  plt.savefig('outputs/visualizations/projection_alignment.png')

Good alignment: Vision and text clusters overlap
Bad alignment: Completely separate clusters
```

---

### **Task 15: Advanced Cross-Attention Deep Dive**

#### **15.1 Cross-Attention in VLM Context**

**15.1.1 Role in Multimodal Understanding**

```
Cross-Attention is THE KEY mechanism for VLM:

What it does:
  - Connects language and vision modalities
  - Allows text to query relevant visual information
  - Grounds language in visual context

Example Flow:
  Input: Image + "Is there a pedestrian?"

  Vision Encoder: Extract 196 patch features

  Text Decoder processes "Is":
    - Self-attention: Relate to previous context
    - Cross-attention: Look at all image patches
    - Output: Hidden state informed by full image

  Text Decoder processes "there":
    - Self-attention: Relate to "Is"
    - Cross-attention: Again look at image
    - Output: Hidden state with visual context

  Text Decoder processes "pedestrian":
    - Self-attention: Relate to "Is there a"
    - Cross-attention: NOW attends strongly to person regions!
    - Output: Hidden state focused on pedestrians in image

  Final token prediction: YES or NO
    - Based on pedestrian-focused visual features

Without cross-attention:
  - Text and vision would be separate
  - No grounding
  - Just blind text generation
```

**15.1.2 Attention Pattern Analysis**

```
What should cross-attention learn?

For command: "Can the car turn left?"

Token "car":
  Should attend to: Car patches in image
  Attention map: High values on car regions

Token "turn":
  Should attend to: Road/lane patches
  Attention map: High on road structure

Token "left":
  Should attend to: Left side of scene
  Attention map: High on left spatial regions

Token "?":
  Should attend to: Broadly (contextual)
  Attention map: Distributed

This is why cross-attention is powerful:
  - Dynamic attention based on query
  - Each word attends to relevant visual regions
  - Emergent grounding behavior
```

#### **15.2 Cross-Attention Implementation Details**

**15.2.1 Memory-Efficient Cross-Attention**

```
Key optimization: Cache vision K, V

Problem:
  Vision features don't change during text generation
  But we recompute K, V every step → wasteful

Solution:
  Compute vision K, V once, cache them

Implementation in GemmaCrossAttention:

def forward(
  self,
  hidden_states,          # Text: [B, text_len, 512]
  encoder_hidden_states,  # Vision: [B, 196, 768]
  past_key_value=None,
  use_cache=False
):

  # Query from text (always recomputed)
  query = self.q_proj(hidden_states)  # [B, text_len, 512]

  # Key, Value from vision (cache if possible)
  if past_key_value is None:
    # First time: compute K, V from vision
    key = self.k_proj(encoder_hidden_states)
    value = self.v_proj(encoder_hidden_states)
  else:
    # Reuse cached K, V
    key, value = past_key_value

  # ... rest of attention computation

  if use_cache:
    return output, attention_weights, (key, value)  # Cache K, V
  else:
    return output, attention_weights

Memory Savings:
  Without cache: Recompute K,V for 196 patches every step
  With cache: Compute once, reuse

  For generation of 50 tokens: 50x speedup for K,V computation!
```

**15.2.2 Cross-Attention vs Self-Attention**

```
Comparison Table:

Aspect              | Self-Attention        | Cross-Attention
--------------------|-----------------------|------------------------
Query source        | Same sequence         | Decoder (text)
Key source          | Same sequence         | Encoder (vision)
Value source        | Same sequence         | Encoder (vision)
Mask type           | Causal (decoder)      | None (full attention)
Attention matrix    | [text_len, text_len]  | [text_len, 196]
Purpose             | Relate tokens to each other | Ground text in vision
KV-cache behavior   | Grows each step       | Fixed (vision cached)

Example shapes:
  Self-Attention:
    Q: [B, 8, 20, 64]  (8 heads, 20 text tokens)
    K: [B, 8, 20, 64]
    V: [B, 8, 20, 64]
    Attn: [B, 8, 20, 20]

  Cross-Attention:
    Q: [B, 8, 20, 64]  (from text)
    K: [B, 8, 196, 64] (from vision)
    V: [B, 8, 196, 64]
    Attn: [B, 8, 20, 196]  ← Different shape!
```

#### **15.3 Attention Weight Interpretation**

**15.3.1 Extracting Cross-Attention Maps**

```
During inference, extract attention to visualize:

def extract_cross_attention_maps(model, image, command):
  """
  Extract cross-attention weights for visualization.

  Returns attention from each text token to each image patch.
  """
  # Forward pass with attention output
  outputs = model(
    pixel_values=image,
    input_ids=command,
    output_attentions=True
  )

  # Get cross-attention weights from all layers
  cross_attentions = outputs["cross_attentions"]
  # List of [B, num_heads, text_len, 196]

  # Average over heads and layers
  avg_attention = torch.stack(cross_attentions).mean(dim=[0, 1, 2])
  # Shape: [text_len, 196]

  return avg_attention

Visualization:
  attention_map = extract_cross_attention_maps(model, img, cmd)

  # For token "pedestrian" (index 5)
  pedestrian_attention = attention_map[5]  # [196]

  # Reshape to 2D (14x14 grid)
  attention_2d = pedestrian_attention.reshape(14, 14)

  # Overlay on image
  import matplotlib.pyplot as plt
  plt.imshow(image)
  plt.imshow(attention_2d, alpha=0.5, cmap='hot')
  plt.title('Attention for "pedestrian"')
```

**15.3.2 Common Attention Patterns**

```
Pattern 1: Uniform Attention (Early Training)
  All tokens attend uniformly to all patches
  Indicates: Model hasn't learned alignment yet

  Attention map: All values ≈ 1/196

Pattern 2: Localized Attention (Object Words)
  Noun tokens attend to specific regions
  Indicates: Model is grounding nouns

  Example: "car" → high attention on car patches

Pattern 3: Distributed Attention (Function Words)
  Articles, prepositions attend broadly
  Indicates: Contextual information gathering

  Example: "is", "the" → uniform attention

Pattern 4: Spatial Attention (Direction Words)
  Direction words attend to specific locations
  Indicates: Spatial reasoning

  Example: "left" → high attention on left side

Pattern 5: No Attention (Padding)
  Padding tokens should have zero attention
  Check: Ensure mask is working correctly
```

#### **15.4 Advanced Cross-Attention Variants**

**15.4.1 Multi-Scale Cross-Attention**

```
Use vision features from multiple resolutions:

class MultiScaleCrossAttention(nn.Module):
  def __init__(self, config):
    super().__init__()

    # Attend to multiple levels of vision pyramid
    self.cross_attn_layer1 = CrossAttention(config)  # 14x14 patches
    self.cross_attn_layer2 = CrossAttention(config)  # 7x7 patches
    self.cross_attn_layer3 = CrossAttention(config)  # Global

    # Combine outputs
    self.fusion = nn.Linear(config.hidden_size * 3, config.hidden_size)

  def forward(self, text_hidden, vision_features_pyramid):
    # Attend to each scale
    out1 = self.cross_attn_layer1(text_hidden, vision_features_pyramid[0])
    out2 = self.cross_attn_layer2(text_hidden, vision_features_pyramid[1])
    out3 = self.cross_attn_layer3(text_hidden, vision_features_pyramid[2])

    # Concatenate and fuse
    combined = torch.cat([out1, out2, out3], dim=-1)
    output = self.fusion(combined)

    return output

Benefits:
  - Fine details from high-res
  - Context from low-res
  - Better for multi-scale objects

For this project: Single-scale sufficient
```

**15.4.2 Sparse Cross-Attention**

```
For efficiency, attend to subset of patches:

class SparseCrossAttention(nn.Module):
  def __init__(self, config):
    super().__init__()
    self.top_k = config.cross_attention_top_k  # e.g., 64
    self.cross_attn = CrossAttention(config)

  def forward(self, text_hidden, vision_features):
    # First pass: compute attention scores
    scores = compute_attention_scores(text_hidden, vision_features)
    # Shape: [B, num_heads, text_len, 196]

    # Select top-k patches per query
    top_k_indices = scores.topk(self.top_k, dim=-1).indices

    # Gather top-k vision features
    selected_vision = gather_by_indices(vision_features, top_k_indices)

    # Full attention on selected patches only
    output = self.cross_attn(text_hidden, selected_vision)

    return output

Benefits:
  - Faster (196 → 64 patches)
  - Less memory
  - Focus on relevant regions

Drawbacks:
  - Potential information loss
  - More complex implementation

For this project: Full attention (196 patches is manageable)
```

#### **15.5 Cross-Attention Debugging**

**15.5.1 Common Issues**

```
Issue 1: All-zero attention
  Symptom: All attention weights are zero
  Cause: Incorrect masking (everything masked out)
  Fix: Check encoder_attention_mask is correct

Issue 2: Uniform attention (not learning)
  Symptom: All patches get equal attention (1/196)
  Cause: Insufficient training or learning rate too low
  Fix: Train longer, increase LR for cross-attention layers

Issue 3: Attention collapse
  Symptom: All queries attend to same patch
  Cause: Gradient explosion or NaN values
  Fix: Check for NaN, reduce LR, add gradient clipping

Issue 4: No gradient flow
  Symptom: Vision encoder not learning
  Cause: Detached gradients or frozen layers
  Fix: Check requires_grad=True for all modules

Issue 5: Memory overflow
  Symptom: OOM during cross-attention
  Cause: Large batch × text_len × 196 attention matrix
  Fix: Reduce batch size, use gradient checkpointing
```

**15.5.2 Debugging Tools**

```
Tool 1: Attention Statistics
  def print_attention_stats(attention_weights):
    print(f"Shape: {attention_weights.shape}")
    print(f"Min: {attention_weights.min().item():.6f}")
    print(f"Max: {attention_weights.max().item():.6f}")
    print(f"Mean: {attention_weights.mean().item():.6f}")
    print(f"Std: {attention_weights.std().item():.6f}")
    print(f"Any NaN: {torch.isnan(attention_weights).any()}")
    print(f"Any Inf: {torch.isinf(attention_weights).any()}")

Tool 2: Gradient Monitoring
  def check_cross_attn_gradients(model):
    for name, param in model.named_parameters():
      if "cross_attn" in name and param.grad is not None:
        grad_norm = param.grad.norm().item()
        print(f"{name}: grad_norm={grad_norm:.6f}")

Tool 3: Attention Entropy
  def compute_attention_entropy(attention_weights):
    # High entropy = uniform, Low entropy = peaked
    # attention_weights: [B, heads, text_len, 196]
    entropy = -(attention_weights * torch.log(attention_weights + 1e-10)).sum(dim=-1)
    return entropy.mean().item()

  # Good: entropy between 3 and 5 (for 196 patches, max is log(196)≈5.3)
  # Too high: uniform attention (not learning)
  # Too low: attention collapse
```

---

### **Task 16: multimodal_fusion.py - Combining Vision and Language**

#### **16.1 Fusion Strategy Overview**

**16.1.1 Fusion Approaches in VLMs**

```
Approach 1: Early Fusion
  Combine vision and language at input level

  [Image features] + [Text embeddings] → Joint Encoder

  Pros: Deep integration, shared representations
  Cons: Fixed at input, less flexible

Approach 2: Late Fusion
  Process separately, combine at output

  Vision Encoder → Vision features
  Language Encoder → Language features
  Fusion → Combined → Classifier

  Pros: Modular, can use pretrained encoders
  Cons: Limited interaction

Approach 3: Cross-Modal Attention (PaliGemma style) ✓
  Vision encoder processes image
  Language decoder attends to vision via cross-attention

  Vision Encoder → Visual tokens
  Language Decoder (with cross-attn) → Output

  Pros: Flexible, deep integration, efficient
  Cons: More complex

For this project: Cross-Modal Attention (Task 15)
  - Already implemented in decoder layers
  - This task focuses on integration
```

**16.1.2 Fusion Configuration**

```
FusionConfig:
  # Modality dimensions
  vision_hidden_size: 768
  language_hidden_size: 512

  # Fusion method
  fusion_type: "cross_attention"  # or "concat", "add"

  # Cross-attention config
  use_cross_attention: True
  cross_attention_frequency: 1  # Every layer

  # Alternative fusion (if not using cross-attention)
  fusion_hidden_size: 512
  fusion_num_layers: 2

For this project: Cross-attention every decoder layer
```

#### **16.2 Alternative Fusion Methods**

**16.2.1 Concatenation Fusion**

```
Simplest approach: Concatenate vision and text sequences

class ConcatFusion(nn.Module):
  def __init__(self, config):
    super().__init__()

    # Project vision to language dimension
    self.vision_projection = nn.Linear(
      config.vision_hidden_size,
      config.language_hidden_size
    )

  def forward(self, vision_features, text_embeddings):
    """
    Args:
      vision_features: [B, 196, 768]
      text_embeddings: [B, text_len, 512]

    Returns:
      fused: [B, 196 + text_len, 512]
    """
    # Project vision
    projected_vision = self.vision_projection(vision_features)
    # [B, 196, 512]

    # Concatenate along sequence dimension
    fused = torch.cat([projected_vision, text_embeddings], dim=1)
    # [B, 196 + text_len, 512]

    return fused

Usage:
  fused_sequence = concat_fusion(vision_features, text_embeddings)

  # Pass through decoder with self-attention only
  decoder_output = decoder(fused_sequence)

  # Take text portion for output
  text_output = decoder_output[:, 196:, :]

Attention Mask:
  # Vision tokens can see all
  # Text tokens can see vision + previous text (causal)

  mask = [
    [1, 1, ..., 1, 1, 1, ..., 1],  # Vision token 0: sees all
    [1, 1, ..., 1, 1, 1, ..., 1],  # Vision token 195: sees all
    [1, 1, ..., 1, 1, 0, ..., 0],  # Text token 0: sees vision + self
    [1, 1, ..., 1, 1, 1, 0, ..., 0],  # Text token 1: sees vision + 0,1
    ...
  ]

Pros:
  - Simple to implement
  - Single self-attention enough
  - Proven in PaLM-E

Cons:
  - Longer sequences (196 + text_len)
  - Quadratic attention cost
  - Less efficient for generation

For this project: Not using (cross-attention is better)
```

**16.2.2 Additive Fusion**

```
Add vision context to text embeddings:

class AdditiveFusion(nn.Module):
  def __init__(self, config):
    super().__init__()

    # Pool vision features to single vector
    self.vision_pooler = nn.Sequential(
      nn.Linear(config.vision_hidden_size, config.language_hidden_size),
      nn.Tanh()
    )

    # Attention pooling
    self.attention_weights = nn.Linear(config.vision_hidden_size, 1)

  def forward(self, vision_features, text_embeddings):
    """
    Args:
      vision_features: [B, 196, 768]
      text_embeddings: [B, text_len, 512]

    Returns:
      fused: [B, text_len, 512]
    """
    # Attention-weighted pooling of vision
    attn_scores = self.attention_weights(vision_features)  # [B, 196, 1]
    attn_weights = F.softmax(attn_scores, dim=1)  # [B, 196, 1]

    pooled_vision = (vision_features * attn_weights).sum(dim=1)  # [B, 768]
    pooled_vision = self.vision_pooler(pooled_vision)  # [B, 512]

    # Broadcast and add to text
    pooled_vision = pooled_vision.unsqueeze(1)  # [B, 1, 512]
    fused = text_embeddings + pooled_vision  # [B, text_len, 512]

    return fused

Pros:
  - Compact representation
  - Fast (no attention overhead)
  - Simple

Cons:
  - Loses spatial information
  - Single global vision context
  - Can't attend to specific regions

For this project: Not using (too limited for spatial reasoning)
```

**16.2.3 Gated Fusion**

```
Learn to gate between vision and language:

class GatedFusion(nn.Module):
  def __init__(self, config):
    super().__init__()

    self.vision_transform = nn.Linear(config.vision_hidden_size, config.hidden_size)
    self.text_transform = nn.Linear(config.hidden_size, config.hidden_size)

    # Gating mechanism
    self.gate = nn.Linear(config.hidden_size * 2, config.hidden_size)

  def forward(self, vision_features, text_features):
    """
    Dynamic gating between modalities.

    Args:
      vision_features: [B, 196, 768] or pooled [B, 768]
      text_features: [B, hidden_size]

    Returns:
      fused: [B, hidden_size]
    """
    # Transform to same space
    v = self.vision_transform(vision_features)  # [B, hidden_size]
    t = self.text_transform(text_features)  # [B, hidden_size]

    # Compute gate
    concat = torch.cat([v, t], dim=-1)  # [B, hidden_size * 2]
    gate = torch.sigmoid(self.gate(concat))  # [B, hidden_size]

    # Gated combination
    fused = gate * v + (1 - gate) * t

    return fused

Interpretation:
  gate ≈ 1 → use vision
  gate ≈ 0 → use text
  gate ≈ 0.5 → balance both

Useful for: Final classification layer
```

#### **16.3 Vision-Language Alignment**

**16.3.1 Contrastive Alignment (Optional)**

```
Add contrastive loss to align vision and language:

Idea: Image and its caption should have similar representations

class ContrastiveAlignment(nn.Module):
  def __init__(self, config):
    super().__init__()

    # Project both to same space
    self.vision_proj = nn.Linear(config.vision_hidden_size, config.embedding_dim)
    self.text_proj = nn.Linear(config.language_hidden_size, config.embedding_dim)

    # Temperature for contrastive loss
    self.temperature = nn.Parameter(torch.ones([]) * 0.07)

  def forward(self, vision_features, text_features):
    """
    Compute contrastive loss (like CLIP).

    Args:
      vision_features: [B, vision_dim] - pooled
      text_features: [B, text_dim] - pooled

    Returns:
      loss: Contrastive loss
    """
    # Project to embedding space
    vision_emb = self.vision_proj(vision_features)  # [B, emb_dim]
    text_emb = self.text_proj(text_features)  # [B, emb_dim]

    # Normalize
    vision_emb = F.normalize(vision_emb, dim=-1)
    text_emb = F.normalize(text_emb, dim=-1)

    # Compute similarity matrix
    logits = torch.matmul(vision_emb, text_emb.T) / self.temperature
    # [B, B] - diagonal should be high

    # Labels: each image matches its text
    labels = torch.arange(len(vision_emb), device=logits.device)

    # Symmetric cross-entropy loss
    loss_i2t = F.cross_entropy(logits, labels)
    loss_t2i = F.cross_entropy(logits.T, labels)

    loss = (loss_i2t + loss_t2i) / 2

    return loss

When to use:
  - If training from scratch
  - To improve alignment
  - As auxiliary loss during training

For this project: Not needed initially (cross-attention handles alignment)
```

#### **16.4 Multimodal Representation**

**16.4.1 Pooling Strategies**

```
For classification, need single representation:

Option 1: Last Token (Decoder Style)
  # Use last text token's hidden state
  last_hidden = decoder_output[:, -1, :]  # [B, hidden_size]
  logits = classifier(last_hidden)

Option 2: Mean Pooling
  # Average over sequence
  mean_hidden = decoder_output.mean(dim=1)  # [B, hidden_size]
  logits = classifier(mean_hidden)

Option 3: Max Pooling
  # Max over sequence
  max_hidden = decoder_output.max(dim=1)[0]  # [B, hidden_size]
  logits = classifier(max_hidden)

Option 4: Attention Pooling
  # Learned attention weights
  attention_weights = attention_pooler(decoder_output)  # [B, seq_len, 1]
  attention_weights = F.softmax(attention_weights, dim=1)
  pooled = (decoder_output * attention_weights).sum(dim=1)  # [B, hidden_size]
  logits = classifier(pooled)

Option 5: CLS Token (ViT Style)
  # Prepend [CLS] token, use its final representation
  cls_hidden = decoder_output[:, 0, :]  # [B, hidden_size]
  logits = classifier(cls_hidden)

For this project: Use Last Token (standard for decoder-only)
```

**16.4.2 Multimodal Classifier Head**

```
Classification from multimodal representation:

class MultimodalClassifier(nn.Module):
  def __init__(self, config):
    super().__init__()

    self.hidden_size = config.language_hidden_size
    self.num_classes = config.num_classes  # 2 for YES/NO

    # Optional: Additional fusion layer
    self.fusion_layer = nn.Sequential(
      nn.Linear(self.hidden_size, self.hidden_size),
      nn.LayerNorm(self.hidden_size),
      nn.GELU(),
      nn.Dropout(0.1)
    )

    # Classification head
    self.classifier = nn.Linear(self.hidden_size, self.num_classes)

  def forward(self, hidden_states):
    """
    Args:
      hidden_states: [B, seq_len, hidden_size] or [B, hidden_size]

    Returns:
      logits: [B, num_classes]
    """
    # If sequence, take last token
    if hidden_states.ndim == 3:
      hidden_states = hidden_states[:, -1, :]  # [B, hidden_size]

    # Optional fusion
    hidden_states = self.fusion_layer(hidden_states)

    # Classify
    logits = self.classifier(hidden_states)  # [B, num_classes]

    return logits

Usage:
  decoder_output = decoder(input_ids, encoder_hidden_states=vision_features)
  last_hidden = decoder_output["last_hidden_state"]
  logits = classifier(last_hidden)
  predictions = torch.argmax(logits, dim=-1)
```

#### **16.5 Testing Fusion Components**

**16.5.1 Fusion Tests**

```
1. test_projection_then_crossattn():
   # End-to-end test
   vision_features = torch.randn(2, 196, 768)
   text_ids = torch.randint(0, 500, (2, 20))

   # Project vision
   projected_vision = projection(vision_features)  # [2, 196, 512]

   # Decoder with cross-attention
   decoder_output = decoder(
     text_ids,
     encoder_hidden_states=projected_vision
   )

   assert decoder_output["last_hidden_state"].shape == (2, 20, 512)

2. test_multimodal_classification():
   decoder_output = decoder(text_ids, encoder_hidden_states=projected_vision)
   logits = classifier(decoder_output["last_hidden_state"])

   assert logits.shape == (2, 2)  # [B, num_classes]

   # Check probabilities sum to 1
   probs = F.softmax(logits, dim=-1)
   assert torch.allclose(probs.sum(dim=-1), torch.ones(2))

3. test_gradient_flow_through_fusion():
   vision_features = torch.randn(1, 196, 768, requires_grad=True)
   text_ids = torch.randint(0, 500, (1, 10))
   labels = torch.tensor([1])

   projected = projection(vision_features)
   decoder_output = decoder(text_ids, encoder_hidden_states=projected)
   logits = classifier(decoder_output["last_hidden_state"])

   loss = F.cross_entropy(logits, labels)
   loss.backward()

   # Check gradients reach vision encoder
   assert vision_features.grad is not None
   assert not torch.isnan(vision_features.grad).any()
```

---

### **Task 17: vlm_model.py - Complete VLM Assembly**

#### **17.1 Complete VLM Architecture**

**17.1.1 TrafficVLM Class Structure**

```
Class: TrafficVLM(nn.Module)

Purpose: Integrate all components into complete VLM

Components:
  1. Vision Encoder (SigLip)
  2. Vision-Language Projection
  3. Language Decoder (Gemma)
  4. Classification Head

Attributes:
  - config: VLMConfig
  - vision_encoder: SigLipVisionEncoder
  - projection: VisionLanguageProjection
  - decoder: GemmaDecoder
  - classifier: nn.Linear or MultimodalClassifier

Methods:
  - __init__(config)
  - forward(pixel_values, input_ids, attention_mask, labels)
  - generate(pixel_values, input_ids, max_length)
  - get_vision_features(pixel_values)
  - freeze_vision_encoder()
  - unfreeze_vision_encoder()
```

**17.1.2 Complete VLM Configuration**

```
@dataclass
class VLMConfig:
  # Vision Encoder Config
  image_size: int = 224
  patch_size: int = 16
  vision_hidden_size: int = 768
  vision_num_layers: int = 6
  vision_num_heads: int = 12

  # Projection Config
  projection_intermediate_size: int = 1024

  # Language Decoder Config
  vocab_size: int = 500
  language_hidden_size: int = 512
  decoder_num_layers: int = 4
  decoder_num_heads: int = 8
  decoder_num_kv_heads: int = 2
  decoder_intermediate_size: int = 2048
  max_position_embeddings: int = 128

  # Classification
  num_classes: int = 2  # YES/NO

  # Training
  hidden_dropout: float = 0.1
  attention_dropout: float = 0.0

  # Initialization
  initializer_range: float = 0.02

  # RoPE
  rope_theta: float = 10000.0

  # Normalization
  rms_norm_eps: float = 1e-6
  layer_norm_eps: float = 1e-6

  # Optimization
  tie_word_embeddings: bool = True
  use_cache: bool = True

  def __post_init__(self):
    # Create sub-configs
    self.vision_config = VisionEncoderConfig(
      image_size=self.image_size,
      patch_size=self.patch_size,
      hidden_size=self.vision_hidden_size,
      num_hidden_layers=self.vision_num_layers,
      num_attention_heads=self.vision_num_heads
    )

    self.decoder_config = GemmaDecoderConfig(
      vocab_size=self.vocab_size,
      hidden_size=self.language_hidden_size,
      num_hidden_layers=self.decoder_num_layers,
      num_attention_heads=self.decoder_num_heads,
      num_key_value_heads=self.decoder_num_kv_heads,
      intermediate_size=self.decoder_intermediate_size
    )
```

**17.1.3 TrafficVLM Implementation**

```
class TrafficVLM(nn.Module):
  def __init__(self, config: VLMConfig):
    super().__init__()

    self.config = config

    # ===== Vision Encoder =====
    self.vision_encoder = SigLipVisionEncoder(config.vision_config)

    # ===== Projection Layer =====
    self.vision_projection = VisionLanguageProjection(
      vision_hidden_size=config.vision_hidden_size,
      language_hidden_size=config.language_hidden_size,
      intermediate_size=config.projection_intermediate_size
    )

    # ===== Language Decoder =====
    self.decoder = GemmaDecoder(config.decoder_config)

    # ===== Classification Head =====
    # Option 1: Direct from vocab logits
    self.lm_head = nn.Linear(
      config.language_hidden_size,
      config.vocab_size,
      bias=False
    )

    # Weight tying
    if config.tie_word_embeddings:
      self.lm_head.weight = self.decoder.embed_tokens.weight

    # Option 2: Separate classifier (can use both)
    self.classifier = nn.Linear(config.language_hidden_size, config.num_classes)

    # Initialize weights
    self.apply(self._init_weights)

    print(f"Initialized TrafficVLM with {self.count_parameters():,} parameters")

  def _init_weights(self, module):
    """Initialize model weights."""
    if isinstance(module, nn.Linear):
      nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)
      if module.bias is not None:
        nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
      nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)
    elif isinstance(module, (nn.LayerNorm, RMSNorm)):
      if hasattr(module, 'weight'):
        nn.init.ones_(module.weight)
      if hasattr(module, 'bias') and module.bias is not None:
        nn.init.zeros_(module.bias)

  def count_parameters(self):
    """Count trainable parameters."""
    return sum(p.numel() for p in self.parameters() if p.requires_grad)

  def get_vision_features(self, pixel_values):
    """
    Extract and project vision features.

    Args:
      pixel_values: [B, 3, 224, 224]

    Returns:
      projected_vision: [B, 196, language_hidden_size]
    """
    # Vision encoder
    vision_outputs = self.vision_encoder(pixel_values)
    vision_features = vision_outputs["last_hidden_state"]  # [B, 196, 768]

    # Project to language space
    projected_vision = self.vision_projection(vision_features)  # [B, 196, 512]

    return projected_vision

  def forward(
    self,
    pixel_values: torch.Tensor,              # [B, 3, 224, 224]
    input_ids: torch.Tensor,                 # [B, seq_len]
    attention_mask: Optional[torch.Tensor] = None,
    labels: Optional[torch.Tensor] = None,   # [B] for classification
    output_attentions: bool = False,
    output_hidden_states: bool = False,
    return_dict: bool = True
  ):
    """
    Forward pass through complete VLM.

    Returns:
      dict: {
        "loss": Optional[torch.Tensor],
        "logits": torch.Tensor [B, num_classes] or [B, seq_len, vocab_size],
        "vision_features": torch.Tensor [B, 196, 512],
        "decoder_hidden_states": torch.Tensor [B, seq_len, 512],
        "cross_attentions": Optional[List],
        ...
      }
    """
    batch_size = pixel_values.shape[0]

    # ===== Step 1: Vision Encoding & Projection =====
    projected_vision = self.get_vision_features(pixel_values)
    # [B, 196, 512]

    # ===== Step 2: Language Decoding with Cross-Attention =====
    decoder_outputs = self.decoder(
      input_ids=input_ids,
      attention_mask=attention_mask,
      encoder_hidden_states=projected_vision,
      output_attentions=output_attentions,
      output_hidden_states=output_hidden_states,
      use_cache=False  # Don't cache during training
    )

    hidden_states = decoder_outputs["last_hidden_state"]  # [B, seq_len, 512]

    # ===== Step 3: Classification =====
    # Take last token representation
    last_hidden = hidden_states[:, -1, :]  # [B, 512]

    # Classification logits
    classification_logits = self.classifier(last_hidden)  # [B, 2]

    # ===== Step 4: Compute Loss (if labels provided) =====
    loss = None
    if labels is not None:
      loss_fct = nn.CrossEntropyLoss()
      loss = loss_fct(classification_logits, labels)

    # ===== Step 5: Prepare Output =====
    if not return_dict:
      output = (classification_logits,) + decoder_outputs[1:]
      return ((loss,) + output) if loss is not None else output

    return {
      "loss": loss,
      "logits": classification_logits,
      "vision_features": projected_vision,
      "decoder_hidden_states": hidden_states,
      "past_key_values": decoder_outputs.get("past_key_values"),
      "attentions": decoder_outputs.get("attentions"),
      "cross_attentions": decoder_outputs.get("cross_attentions")
    }
```

#### **17.2 Training Mode vs Inference Mode**

**17.2.1 Training Forward Pass**

```
During training:

# Single forward pass for loss computation
outputs = model(
  pixel_values=images,      # [B, 3, 224, 224]
  input_ids=command_ids,    # [B, seq_len]
  attention_mask=attn_mask, # [B, seq_len]
  labels=labels             # [B] - 0 or 1
)

loss = outputs["loss"]
logits = outputs["logits"]  # [B, 2]

# Backward
loss.backward()
optimizer.step()

Key Points:
  - All tokens processed in parallel
  - Cross-attention to vision every layer
  - Classification from last token
  - No KV-cache (not needed for parallel processing)
```

**17.2.2 Inference Forward Pass**

```
During inference (same as training for classification):

with torch.no_grad():
  outputs = model(
    pixel_values=image,
    input_ids=command_ids,
    attention_mask=attn_mask
  )

  logits = outputs["logits"]  # [B, 2]
  predictions = torch.argmax(logits, dim=-1)  # [B]

  # Convert to text
  answers = [tokenizer.id_to_label[pred.item()] for pred in predictions]
  # ["YES", "NO", ...]

For generation (if implementing text generation):
  Use generate() method with KV-cache (see below)
```

#### **17.3 Text Generation (Optional)**

**17.3.1 Autoregressive Generation**

```
def generate(
  self,
  pixel_values: torch.Tensor,
  input_ids: torch.Tensor,
  max_length: int = 50,
  temperature: float = 1.0,
  top_p: float = 0.9,
  do_sample: bool = False
):
  """
  Generate text autoregressively.

  Args:
    pixel_values: [B, 3, 224, 224]
    input_ids: [B, prompt_len] - initial tokens (e.g., [SOS])
    max_length: Maximum generation length
    temperature: Sampling temperature
    top_p: Nucleus sampling threshold
    do_sample: If True, sample; else, greedy

  Returns:
    generated_ids: [B, generated_len]
  """
  batch_size = pixel_values.shape[0]
  device = pixel_values.device

  # Get vision features (constant during generation)
  projected_vision = self.get_vision_features(pixel_values)
  # [B, 196, 512]

  # Initialize generation
  generated = input_ids.clone()  # [B, prompt_len]
  past_key_values = None

  for step in range(max_length):
    # Forward pass (only new token if using cache)
    if past_key_values is None:
      # First step: process entire prompt
      decoder_input = generated
    else:
      # Subsequent steps: only last token
      decoder_input = generated[:, -1:]

    decoder_outputs = self.decoder(
      input_ids=decoder_input,
      encoder_hidden_states=projected_vision,
      past_key_values=past_key_values,
      use_cache=True
    )

    hidden_states = decoder_outputs["last_hidden_state"]
    past_key_values = decoder_outputs["past_key_values"]

    # Get logits for next token
    next_token_logits = self.lm_head(hidden_states[:, -1, :])  # [B, vocab_size]

    # Apply temperature
    next_token_logits = next_token_logits / temperature

    # Sample or greedy
    if do_sample:
      # Nucleus sampling
      probs = F.softmax(next_token_logits, dim=-1)
      sorted_probs, sorted_indices = torch.sort(probs, descending=True)
      cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

      # Remove tokens with cumulative probability > top_p
      sorted_indices_to_remove = cumulative_probs > top_p
      sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
      sorted_indices_to_remove[:, 0] = False

      indices_to_remove = sorted_indices[sorted_indices_to_remove]
      next_token_logits[:, indices_to_remove] = float('-inf')

      # Sample
      probs = F.softmax(next_token_logits, dim=-1)
      next_token = torch.multinomial(probs, num_samples=1)
    else:
      # Greedy
      next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

    # Append to generated sequence
    generated = torch.cat([generated, next_token], dim=1)

    # Check for EOS token
    if (next_token == self.config.eos_token_id).all():
      break

  return generated

Usage:
  generated_ids = model.generate(
    pixel_values=image,
    input_ids=torch.tensor([[tokenizer.sos_token_id]]),
    max_length=30
  )

  text = tokenizer.decode(generated_ids[0])
  print(f"Generated: {text}")

For this project: Not needed (classification only)
  But good to have for future extensions
```

#### **17.4 Model Utilities**

**17.4.1 Freezing/Unfreezing Components**

```
def freeze_vision_encoder(self):
  """Freeze vision encoder weights."""
  for param in self.vision_encoder.parameters():
    param.requires_grad = False
  print("Vision encoder frozen")

def unfreeze_vision_encoder(self):
  """Unfreeze vision encoder weights."""
  for param in self.vision_encoder.parameters():
    param.requires_grad = True
  print("Vision encoder unfrozen")

def freeze_decoder(self):
  """Freeze decoder weights (except cross-attention)."""
  for name, param in self.decoder.named_parameters():
    if "cross_attn" not in name:
      param.requires_grad = False
  print("Decoder frozen (except cross-attention)")

def get_trainable_parameters(self):
  """Get number of trainable parameters by component."""
  counts = {
    "vision_encoder": sum(p.numel() for p in self.vision_encoder.parameters() if p.requires_grad),
    "projection": sum(p.numel() for p in self.vision_projection.parameters() if p.requires_grad),
    "decoder": sum(p.numel() for p in self.decoder.parameters() if p.requires_grad),
    "classifier": sum(p.numel() for p in self.classifier.parameters() if p.requires_grad)
  }
  counts["total"] = sum(counts.values())
  return counts

Training Strategy:
  # Phase 1: Train projection + decoder only
  model.freeze_vision_encoder()
  train(epochs=5, lr=1e-4)

  # Phase 2: Fine-tune everything
  model.unfreeze_vision_encoder()
  train(epochs=10, lr=1e-5)  # Lower LR
```

**17.4.2 Model Saving/Loading**

```
def save_model(model, path):
  """Save complete model."""
  torch.save({
    "model_state_dict": model.state_dict(),
    "config": model.config,
  }, path)
  print(f"Model saved to {path}")

def load_model(path, device="cuda"):
  """Load complete model."""
  checkpoint = torch.load(path, map_location=device)

  config = checkpoint["config"]
  model = TrafficVLM(config)
  model.load_state_dict(checkpoint["model_state_dict"])
  model.to(device)
  model.eval()

  print(f"Model loaded from {path}")
  return model

Usage:
  # Save
  save_model(model, "checkpoints/traffic_vlm_epoch10.pt")

  # Load
  model = load_model("checkpoints/traffic_vlm_epoch10.pt")
```

**17.4.3 Model Summary**

```
def print_model_summary(model):
  """Print detailed model summary."""
  print("=" * 80)
  print("TRAFFIC VLM MODEL SUMMARY")
  print("=" * 80)

  params = model.get_trainable_parameters()
  print(f"\nParameter Counts:")
  print(f"  Vision Encoder:  {params['vision_encoder']:>12,}")
  print(f"  Projection:      {params['projection']:>12,}")
  print(f"  Decoder:         {params['decoder']:>12,}")
  print(f"  Classifier:      {params['classifier']:>12,}")
  print(f"  {'─' * 40}")
  print(f"  Total:           {params['total']:>12,}")

  memory = params['total'] * 4 / (1024**2)  # FP32 MB
  print(f"\nMemory Footprint:")
  print(f"  FP32:  {memory:.2f} MB")
  print(f"  FP16:  {memory/2:.2f} MB")

  print(f"\nArchitecture:")
  print(f"  Image Size:     {model.config.image_size}×{model.config.image_size}")
  print(f"  Patch Size:     {model.config.patch_size}×{model.config.patch_size}")
  print(f"  Patches:        {(model.config.image_size//model.config.patch_size)**2}")
  print(f"  Vision Layers:  {model.config.vision_num_layers}")
  print(f"  Decoder Layers: {model.config.decoder_num_layers}")
  print(f"  Vocab Size:     {model.config.vocab_size}")
  print(f"  Num Classes:    {model.config.num_classes}")

  print("=" * 80)

Example Output:
  ================================================================================
  TRAFFIC VLM MODEL SUMMARY
  ================================================================================

  Parameter Counts:
    Vision Encoder:     22,451,712
    Projection:          1,312,256
    Decoder:            20,234,752
    Classifier:              1,024
    ────────────────────────────────────────
    Total:              43,999,744

  Memory Footprint:
    FP32:  167.85 MB
    FP16:  83.93 MB

  Architecture:
    Image Size:     224×224
    Patch Size:     16×16
    Patches:        196
    Vision Layers:  6
    Decoder Layers: 4
    Vocab Size:     500
    Num Classes:    2
  ================================================================================
```

#### **17.5 Complete Integration Test**

**17.5.1 End-to-End Test**

```
def test_complete_vlm():
  """Test complete VLM pipeline."""
  print("Testing Complete VLM...")

  # Config
  config = VLMConfig(
    image_size=224,
    vision_hidden_size=768,
    language_hidden_size=512,
    vocab_size=500,
    num_classes=2
  )

  # Initialize model
  model = TrafficVLM(config)
  model.eval()

  # Print summary
  print_model_summary(model)

  # Dummy data
  batch_size = 2
  pixel_values = torch.randn(batch_size, 3, 224, 224)
  input_ids = torch.randint(0, 500, (batch_size, 15))
  attention_mask = torch.ones_like(input_ids)
  labels = torch.randint(0, 2, (batch_size,))

  # Forward pass
  print("\n Testing forward pass...")
  outputs = model(
    pixel_values=pixel_values,
    input_ids=input_ids,
    attention_mask=attention_mask,
    labels=labels,
    output_attentions=True
  )

  # Check outputs
  print(f"  Loss: {outputs['loss'].item():.4f}")
  print(f"  Logits shape: {outputs['logits'].shape}")
  print(f"  Vision features shape: {outputs['vision_features'].shape}")
  print(f"  Decoder hidden states shape: {outputs['decoder_hidden_states'].shape}")

  # Test backward
  print("\nTesting backward pass...")
  outputs['loss'].backward()
  print("  Gradients computed successfully!")

  # Check gradients
  has_grad = sum(1 for p in model.parameters() if p.grad is not None)
  total_params = sum(1 for p in model.parameters())
  print(f"  Parameters with gradients: {has_grad}/{total_params}")

  # Test predictions
  print("\nTesting predictions...")
  with torch.no_grad():
    outputs = model(pixel_values, input_ids, attention_mask)
    predictions = torch.argmax(outputs['logits'], dim=-1)
    probs = F.softmax(outputs['logits'], dim=-1)

  print(f"  Predictions: {predictions.tolist()}")
  print(f"  Probabilities: {probs.tolist()}")

  print("\n✓ All tests passed!")

Run this before training!
```

**17.5.2 Performance Benchmarking**

```
def benchmark_model(model, batch_size=4, num_iterations=100):
  """Benchmark model speed."""
  import time

  model.eval()
  device = next(model.parameters()).device

  # Dummy data
  pixel_values = torch.randn(batch_size, 3, 224, 224).to(device)
  input_ids = torch.randint(0, 500, (batch_size, 20)).to(device)

  # Warmup
  for _ in range(10):
    with torch.no_grad():
      _ = model(pixel_values, input_ids)

  # Benchmark
  torch.cuda.synchronize() if device.type == "cuda" else None
  start = time.time()

  for _ in range(num_iterations):
    with torch.no_grad():
      _ = model(pixel_values, input_ids)

  torch.cuda.synchronize() if device.type == "cuda" else None
  elapsed = time.time() - start

  samples_per_sec = (num_iterations * batch_size) / elapsed
  ms_per_sample = (elapsed * 1000) / (num_iterations * batch_size)

  print(f"\nBenchmark Results:")
  print(f"  Batch size: {batch_size}")
  print(f"  Iterations: {num_iterations}")
  print(f"  Total time: {elapsed:.2f}s")
  print(f"  Throughput: {samples_per_sec:.2f} samples/sec")
  print(f"  Latency: {ms_per_sample:.2f} ms/sample")

Target Performance (A3000):
  - Batch size 4: ~50-100 samples/sec
  - Latency: ~10-20 ms/sample
```

---

## Summary of Phase 4-5 (Tasks 11-17)

### **Completed Components:**

1. **Task 11: Gemma Decoder** ✓

   - Decoder-only architecture
   - 4-layer transformer with GQA
   - Causal self-attention
   - Cross-attention to vision
   - RMS normalization
   - ~20M parameters

2. **Task 12: Decoder Layer** ✓

   - Self-attention block
   - Cross-attention block
   - SwiGLU feed-forward
   - Three residual connections
   - Pre-normalization

3. **Task 13: RoPE Embeddings** ✓

   - Rotary position encoding
   - Relative position information
   - KV-cache support
   - Efficient implementation

4. **Task 14: Projection Layer** ✓

   - Vision (768) → Language (512)
   - Two-layer MLP
   - GELU activation
   - ~1.3M parameters

5. **Task 15: Advanced Cross-Attention** ✓

   - Text queries, vision keys/values
   - Attention map extraction
   - Vision KV caching
   - Grounding visualization

6. **Task 16: Multimodal Fusion** ✓

   - Cross-attention fusion strategy
   - Alternative fusion methods
   - Multimodal representation
   - Classification head

7. **Task 17: Complete VLM Assembly** ✓
   - TrafficVLM class
   - End-to-end forward pass
   - Training and inference modes
   - Model utilities
   - ~44M total parameters

### **Complete Architecture:**

```
Input Image [224×224×3]
    ↓
Vision Encoder (6 layers, 768-dim)
    ↓ [B, 196, 768]
Projection Layer (768→512)
    ↓ [B, 196, 512]
    ┌───────────────┐
    │               │
    │  Text Input   │
    │      ↓        │
    │  Embedding    │
    │      ↓        │
    │  Decoder      │← Cross-Attention
    │  Layer 1-4    │
    │      ↓        │
    └───────────────┘
         ↓ [B, seq_len, 512]
    Last Token [B, 512]
         ↓
    Classifier [B, 2]
         ↓
    YES/NO Prediction
```

### **Memory Footprint:**

- Vision Encoder: 22.5M params (86 MB FP32)
- Projection: 1.3M params (5 MB)
- Decoder: 20.2M params (77 MB)
- Total: 44M params (168 MB FP32, 84 MB FP16)
- Activations (batch=4): ~300 MB
- **Total VRAM needed: ~500 MB (fits in 6GB A3000!)**

### **What's Next:**

Ready to move to Training Pipeline (Tasks 18-25)?

- Loss functions
- Optimizer setup
- Training loop
- Evaluation metrics
- Logging & checkpointing
