# Traffic Scene VLM - ULTRA-DETAILED Tasks 18-25

---

## **PHASE 6: TRAINING PIPELINE (Day 5-6)**

---

### **Task 18: loss_functions.py - Training Objectives**

#### **18.1 Loss Function Design**

**18.1.1 Primary Loss: Cross-Entropy Classification**

```
For binary classification (YES/NO):

Loss Function: Cross-Entropy

Formula:
  L = -∑ y_true * log(y_pred)

  For binary:
    L = -[y * log(p) + (1-y) * log(1-p)]

  Where:
    y = true label (0 or 1)
    p = predicted probability for class 1

PyTorch Implementation:
  loss = nn.CrossEntropyLoss()(logits, labels)

  # Equivalent to:
  # probs = F.softmax(logits, dim=-1)
  # loss = -torch.log(probs[range(len(labels)), labels]).mean()

Why Cross-Entropy?
  - Standard for classification
  - Differentiable
  - Penalizes confident wrong predictions heavily
  - Works well with softmax output
```

**18.1.2 Loss Function Class**

```
Class: VLMLoss(nn.Module)

Purpose: Compute training loss with optional auxiliary losses

Attributes:
  - classification_weight: float (1.0)
  - contrastive_weight: float (0.0)
  - label_smoothing: float (0.0)
  - class_weights: Optional[torch.Tensor]

Methods:
  - __init__(config)
  - forward(logits, labels, **kwargs) → dict
  - classification_loss(logits, labels)
  - contrastive_loss(vision_features, text_features)

Implementation:
  class VLMLoss(nn.Module):
    def __init__(self, config):
      super().__init__()

      self.num_classes = config.num_classes
      self.classification_weight = config.classification_loss_weight
      self.label_smoothing = config.label_smoothing

      # Optional: Class weights for imbalanced data
      if config.class_weights is not None:
        self.register_buffer(
          "class_weights",
          torch.tensor(config.class_weights, dtype=torch.float32)
        )
      else:
        self.class_weights = None

      # Classification loss
      self.classification_loss_fn = nn.CrossEntropyLoss(
        weight=self.class_weights,
        label_smoothing=self.label_smoothing
      )

    def forward(
      self,
      logits: torch.Tensor,           # [B, num_classes]
      labels: torch.Tensor,            # [B]
      vision_features: Optional[torch.Tensor] = None,
      text_features: Optional[torch.Tensor] = None
    ):
      """
      Compute total loss.

      Returns:
        dict: {
          "loss": Total loss,
          "classification_loss": Classification component,
          "auxiliary_loss": Optional auxiliary losses
        }
      """
      # Primary classification loss
      classification_loss = self.classification_loss_fn(logits, labels)

      # Total loss (start with classification)
      total_loss = self.classification_weight * classification_loss

      # Prepare output
      output = {
        "loss": total_loss,
        "classification_loss": classification_loss.detach(),
      }

      return output

    def classification_loss(self, logits, labels):
      """Compute cross-entropy loss."""
      return self.classification_loss_fn(logits, labels)

Parameter Count: 0 (loss functions have no learnable parameters)
```

**18.1.3 Label Smoothing**

```
Label Smoothing: Prevents overconfidence

Standard one-hot labels:
  Class 0: [1, 0]
  Class 1: [0, 1]

With label smoothing (ε=0.1):
  Class 0: [0.95, 0.05]  # (1-ε, ε/num_classes)
  Class 1: [0.05, 0.95]

Why use it?
  - Prevents overconfident predictions
  - Better calibration
  - Slight regularization effect
  - Improves generalization

Implementation in PyTorch:
  loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)

Recommended value: 0.0 - 0.1
  - Start with 0.0
  - Add 0.1 if seeing overconfidence
```

**18.1.4 Class Weighting**

```
For imbalanced datasets:

Problem:
  - 3000 YES samples, 1000 NO samples
  - Model biased toward YES
  - Poor performance on NO class

Solution: Class weights

Compute class weights:
  from sklearn.utils.class_weight import compute_class_weight

  class_weights = compute_class_weight(
    'balanced',
    classes=np.array([0, 1]),
    y=train_labels
  )
  # Returns: [weight_class_0, weight_class_1]

  Example output: [0.67, 2.0]
  # Class 1 (minority) gets higher weight

Usage:
  loss_fn = nn.CrossEntropyLoss(
    weight=torch.tensor([0.67, 2.0])
  )

Effect:
  - Loss for minority class weighted higher
  - Model forced to learn both classes
  - Better balanced accuracy

For this project:
  - Check class distribution first
  - If ratio > 2:1, use class weights
  - Otherwise, balanced data is fine
```

#### **18.2 Auxiliary Losses (Optional)**

**18.2.1 Contrastive Vision-Language Loss**

```
Optional: Add CLIP-style contrastive loss

Purpose: Align vision and language representations

Implementation:
  class ContrastiveLoss(nn.Module):
    def __init__(self, temperature=0.07):
      super().__init__()
      self.temperature = temperature

    def forward(self, vision_features, text_features):
      """
      Contrastive loss between vision and text.

      Args:
        vision_features: [B, embed_dim] - pooled vision
        text_features: [B, embed_dim] - pooled text

      Returns:
        loss: Scalar
      """
      # Normalize features
      vision_features = F.normalize(vision_features, dim=-1)
      text_features = F.normalize(text_features, dim=-1)

      # Compute similarity matrix
      logits = torch.matmul(vision_features, text_features.T) / self.temperature
      # [B, B] - diagonal should be high

      # Labels: each sample matches itself
      labels = torch.arange(len(vision_features), device=logits.device)

      # Bidirectional loss
      loss_i2t = F.cross_entropy(logits, labels)
      loss_t2i = F.cross_entropy(logits.T, labels)

      loss = (loss_i2t + loss_t2i) / 2

      return loss

When to use:
  - If training from scratch
  - To improve alignment
  - As auxiliary loss (weight=0.1-0.2)

For this project:
  - Optional, add if main task plateaus
  - Start without it (simpler)
```

**18.2.2 Attention Regularization**

```
Encourage attention to focus on relevant regions:

class AttentionRegularization(nn.Module):
  def __init__(self, lambda_entropy=0.01):
    super().__init__()
    self.lambda_entropy = lambda_entropy

  def forward(self, cross_attentions):
    """
    Regularize cross-attention to be focused (low entropy).

    Args:
      cross_attentions: List of [B, heads, text_len, 196]

    Returns:
      loss: Scalar
    """
    if cross_attentions is None:
      return 0.0

    entropy_loss = 0.0

    for attn_weights in cross_attentions:
      # Compute entropy over image patches (last dim)
      # H = -∑ p * log(p)
      entropy = -(attn_weights * torch.log(attn_weights + 1e-10)).sum(dim=-1)
      # [B, heads, text_len]

      # Average entropy
      entropy_loss += entropy.mean()

    entropy_loss /= len(cross_attentions)

    # Minimize entropy (encourage focused attention)
    return self.lambda_entropy * entropy_loss

When to use:
  - If attention is too diffuse
  - To encourage object grounding
  - Typically not needed initially

For this project: Skip for now
```

#### **18.3 Loss Computation in Training Loop**

**18.3.1 Standard Training Loss**

```
Basic training step:

def training_step(model, batch, loss_fn):
  """Single training step."""

  # Unpack batch
  images = batch["images"]          # [B, 3, 224, 224]
  input_ids = batch["input_ids"]    # [B, seq_len]
  attention_mask = batch["attention_mask"]  # [B, seq_len]
  labels = batch["labels"]          # [B]

  # Forward pass
  outputs = model(
    pixel_values=images,
    input_ids=input_ids,
    attention_mask=attention_mask,
    labels=labels
  )

  # Get loss (computed inside model)
  loss = outputs["loss"]
  logits = outputs["logits"]

  # Compute accuracy for logging
  predictions = torch.argmax(logits, dim=-1)
  accuracy = (predictions == labels).float().mean()

  return {
    "loss": loss,
    "accuracy": accuracy,
    "logits": logits.detach(),
    "predictions": predictions.detach()
  }

Key Points:
  - Loss computed inside model.forward() for convenience
  - Detach logits to avoid memory leak
  - Return metrics for logging
```

**18.3.2 Loss with Gradient Accumulation**

```
For larger effective batch size:

accumulation_steps = 4
optimizer.zero_grad()

for i, batch in enumerate(train_dataloader):
  # Forward pass
  outputs = model(batch["images"], batch["input_ids"], labels=batch["labels"])
  loss = outputs["loss"]

  # Scale loss by accumulation steps
  loss = loss / accumulation_steps

  # Backward pass
  loss.backward()

  # Update weights every accumulation_steps
  if (i + 1) % accumulation_steps == 0:
    # Gradient clipping (optional)
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

    # Optimizer step
    optimizer.step()
    optimizer.zero_grad()

Effective batch size: batch_size × accumulation_steps
  - Batch size 4 × 4 steps = effective batch 16
  - Fits in 6GB VRAM
  - Better gradient estimates
```

**18.3.3 Mixed Precision Training**

```
Use FP16 for faster training:

from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for batch in train_dataloader:
  optimizer.zero_grad()

  # Forward pass in FP16
  with autocast():
    outputs = model(batch["images"], batch["input_ids"], labels=batch["labels"])
    loss = outputs["loss"]

  # Backward pass with scaling
  scaler.scale(loss).backward()

  # Unscale gradients and clip
  scaler.unscale_(optimizer)
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

  # Optimizer step
  scaler.step(optimizer)
  scaler.update()

Benefits:
  - 2x faster training
  - 2x less memory
  - Same accuracy (if done correctly)

Considerations:
  - Watch for gradient underflow
  - Some operations stay in FP32 (normalization)
  - GradScaler handles automatic loss scaling
```

#### **18.4 Testing Loss Functions**

**18.4.1 Loss Function Tests**

```
1. test_classification_loss():
   loss_fn = VLMLoss(config)

   logits = torch.randn(4, 2)
   labels = torch.tensor([0, 1, 1, 0])

   loss_dict = loss_fn(logits, labels)
   loss = loss_dict["loss"]

   assert loss.ndim == 0  # Scalar
   assert loss.item() >= 0  # Non-negative
   assert not torch.isnan(loss)  # No NaN

2. test_loss_backward():
   model = TrafficVLM(config)
   loss_fn = VLMLoss(config)

   images = torch.randn(2, 3, 224, 224)
   input_ids = torch.randint(0, 500, (2, 10))
   labels = torch.tensor([0, 1])

   outputs = model(images, input_ids, labels=labels)
   loss = outputs["loss"]

   # Backward
   loss.backward()

   # Check gradients exist
   for name, param in model.named_parameters():
     if param.requires_grad:
       assert param.grad is not None, f"No gradient for {name}"
       assert not torch.isnan(param.grad).any(), f"NaN gradient for {name}"

3. test_label_smoothing():
   # Without smoothing
   loss_fn1 = nn.CrossEntropyLoss(label_smoothing=0.0)
   logits = torch.tensor([[10.0, -10.0], [-10.0, 10.0]])
   labels = torch.tensor([0, 1])
   loss1 = loss_fn1(logits, labels)

   # With smoothing
   loss_fn2 = nn.CrossEntropyLoss(label_smoothing=0.1)
   loss2 = loss_fn2(logits, labels)

   # Smoothed loss should be slightly higher (less confident)
   assert loss2 > loss1

4. test_class_weights():
   # Imbalanced labels
   logits = torch.randn(10, 2)
   labels = torch.tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])  # 7:3 imbalance

   # Without weights
   loss1 = nn.CrossEntropyLoss()(logits, labels)

   # With weights (higher weight for minority class)
   weights = torch.tensor([0.6, 1.4])
   loss2 = nn.CrossEntropyLoss(weight=weights)(logits, labels)

   # Losses should differ
   assert loss1 != loss2

5. test_gradient_accumulation_equivalence():
   # Single large batch
   model1 = TrafficVLM(config)
   large_batch = torch.randn(8, 3, 224, 224)
   large_ids = torch.randint(0, 500, (8, 10))
   large_labels = torch.randint(0, 2, (8,))

   out1 = model1(large_batch, large_ids, labels=large_labels)
   loss1 = out1["loss"]
   loss1.backward()

   # Two small batches with accumulation
   model2 = TrafficVLM(config)
   model2.load_state_dict(model1.state_dict())

   for i in range(2):
    small_batch = large_batch[i*4:(i+1)*4]
     small_ids = large_ids[i*4:(i+1)*4]
     small_labels = large_labels[i*4:(i+1)*4]

     out2 = model2(small_batch, small_ids, labels=small_labels)
     loss2 = out2["loss"] / 2  # Scale by accumulation steps
     loss2.backward()

   # Gradients should be similar
   for p1, p2 in zip(model1.parameters(), model2.parameters()):
     if p1.grad is not None:
       assert torch.allclose(p1.grad, p2.grad, atol=1e-4)
```

---

### **Task 19: optimizer.py - Optimization Strategy**

#### **19.1 Optimizer Selection**

**19.1.1 AdamW Optimizer**

```
AdamW (Adam with Weight Decay):

Why AdamW?
  - Most popular for transformers
  - Adaptive learning rates per parameter
  - Decoupled weight decay (better than L2 regularization)
  - Works well out-of-the-box

Formula:
  m_t = β1 * m_{t-1} + (1 - β1) * g_t        # First moment
  v_t = β2 * v_{t-1} + (1 - β2) * g_t^2      # Second moment
  m_hat = m_t / (1 - β1^t)                   # Bias correction
  v_hat = v_t / (1 - β2^t)
  θ_t = θ_{t-1} - lr * m_hat / (√v_hat + ε) - lr * λ * θ_{t-1}  # Weight decay

Parameters:
  - lr (learning rate): 1e-4 to 5e-4
  - β1 (momentum): 0.9
  - β2 (RMSprop momentum): 0.999
  - ε (numerical stability): 1e-8
  - weight_decay: 0.01 to 0.1

Implementation:
  from torch.optim import AdamW

  optimizer = AdamW(
    model.parameters(),
    lr=1e-4,
    betas=(0.9, 0.999),
    eps=1e-8,
    weight_decay=0.01
  )
```

**19.1.2 Parameter Groups (Differential Learning Rates)**

```
Different LRs for different components:

def create_optimizer(model, config):
  """
  Create optimizer with parameter groups.

  Strategy:
    - Lower LR for pretrained components (vision encoder)
    - Higher LR for new components (projection, classifier)
    - No weight decay for biases and norms
  """

  # Separate parameters
  vision_params = []
  projection_params = []
  decoder_params = []
  classifier_params = []

  for name, param in model.named_parameters():
    if not param.requires_grad:
      continue

    if "vision_encoder" in name:
      vision_params.append(param)
    elif "projection" in name:
      projection_params.append(param)
    elif "decoder" in name:
      decoder_params.append(param)
    elif "classifier" in name:
      classifier_params.append(param)

  # Parameter groups with different LRs
  optimizer = AdamW([
    {
      "params": vision_params,
      "lr": config.vision_lr,        # 1e-5 (lower)
      "weight_decay": config.weight_decay
    },
    {
      "params": projection_params,
      "lr": config.projection_lr,    # 5e-4 (higher)
      "weight_decay": config.weight_decay
    },
    {
      "params": decoder_params,
      "lr": config.decoder_lr,       # 1e-4 (medium)
      "weight_decay": config.weight_decay
    },
    {
      "params": classifier_params,
      "lr": config.classifier_lr,    # 5e-4 (higher)
      "weight_decay": config.weight_decay
    }
  ])

  return optimizer

Benefits:
  - Faster convergence
  - Better fine-tuning
  - More control

For this project: Start with single LR, add groups if needed
```

**19.1.3 No Weight Decay for Biases/Norms**

```
Best practice: Exclude biases and LayerNorms from weight decay

def get_parameter_groups(model, weight_decay):
  """
  Separate parameters into decay and no_decay groups.
  """
  decay = []
  no_decay = []

  for name, param in model.named_parameters():
    if not param.requires_grad:
      continue

    # No decay for biases and normalization layers
    if len(param.shape) == 1 or name.endswith(".bias") or "norm" in name.lower():
      no_decay.append(param)
    else:
      decay.append(param)

  return [
    {"params": decay, "weight_decay": weight_decay},
    {"params": no_decay, "weight_decay": 0.0}
  ]

optimizer = AdamW(
  get_parameter_groups(model, weight_decay=0.01),
  lr=1e-4
)

Why?
  - Biases don't need regularization
  - Norms have their own constraints
  - Better performance
```

#### **19.2 Learning Rate Scheduling**

**19.2.1 Warmup + Cosine Decay**

```
Best practice for transformers:

Schedule:
  1. Warmup: Linear increase from 0 to max_lr (500-2000 steps)
  2. Cosine decay: Smooth decrease to min_lr (rest of training)

Formula:
  Warmup (t < warmup_steps):
    lr_t = max_lr * (t / warmup_steps)

  Cosine Decay (t >= warmup_steps):
    progress = (t - warmup_steps) / (total_steps - warmup_steps)
    lr_t = min_lr + 0.5 * (max_lr - min_lr) * (1 + cos(π * progress))

Implementation:
  from torch.optim.lr_scheduler import LambdaLR
  import math

  def get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps,
    num_training_steps,
    min_lr_ratio=0.1
  ):
    """
    Create learning rate scheduler.

    Args:
      optimizer: Optimizer instance
      num_warmup_steps: Number of warmup steps
      num_training_steps: Total training steps
      min_lr_ratio: Minimum LR as fraction of max LR

    Returns:
      scheduler: LR scheduler
    """

    def lr_lambda(current_step):
      # Warmup
      if current_step < num_warmup_steps:
        return float(current_step) / float(max(1, num_warmup_steps))

      # Cosine decay
      progress = float(current_step - num_warmup_steps) / float(
        max(1, num_training_steps - num_warmup_steps)
      )
      cosine_decay = 0.5 * (1.0 + math.cos(math.pi * progress))
      return min_lr_ratio + (1.0 - min_lr_ratio) * cosine_decay

    return LambdaLR(optimizer, lr_lambda)

Usage:
  optimizer = AdamW(model.parameters(), lr=1e-4)

  num_epochs = 20
  steps_per_epoch = len(train_dataloader)
  total_steps = num_epochs * steps_per_epoch
  warmup_steps = int(0.1 * total_steps)  # 10% warmup

  scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
  )

  # In training loop:
  for batch in train_dataloader:
    loss = train_step(model, batch)
    loss.backward()
    optimizer.step()
    scheduler.step()  # Update LR every step
    optimizer.zero_grad()
```

**19.2.2 Alternative: Linear Decay**

```
Simpler schedule:

from torch.optim.lr_scheduler import LinearLR

# Warmup
warmup_scheduler = LinearLR(
  optimizer,
  start_factor=0.01,  # Start at 1% of base LR
  end_factor=1.0,     # Reach 100% of base LR
  total_iters=warmup_steps
)

# Decay
decay_scheduler = LinearLR(
  optimizer,
  start_factor=1.0,
  end_factor=0.1,     # End at 10% of base LR
  total_iters=total_steps - warmup_steps
)

# Combine
from torch.optim.lr_scheduler import SequentialLR

scheduler = SequentialLR(
  optimizer,
  schedulers=[warmup_scheduler, decay_scheduler],
  milestones=[warmup_steps]
)

Simpler but less smooth than cosine
```

**19.2.3 Alternative: ReduceLROnPlateau**

```
Adaptive schedule based on validation:

from torch.optim.lr_scheduler import ReduceLROnPlateau

scheduler = ReduceLROnPlateau(
  optimizer,
  mode='max',          # Maximize accuracy
  factor=0.5,          # Reduce LR by 50%
  patience=3,          # Wait 3 epochs
  verbose=True,
  min_lr=1e-7
)

# In training loop (after each epoch):
val_accuracy = evaluate(model, val_dataloader)
scheduler.step(val_accuracy)

Pros:
  - Adaptive to training dynamics
  - No need to set total steps

Cons:
  - Can be unpredictable
  - Less control

For this project: Use cosine with warmup (more stable)
```

#### **19.3 Gradient Clipping**

**19.3.1 Clipping by Norm**

```
Prevent gradient explosion:

Why clip?
  - Transformers prone to gradient spikes
  - Prevents training instability
  - Allows higher learning rates

Methods:
  1. Clip by global norm (recommended)
  2. Clip by value

Implementation:
  import torch.nn.utils as nn_utils

  # Clip by global norm
  max_norm = 1.0
  nn_utils.clip_grad_norm_(model.parameters(), max_norm)

  # Equivalent to:
  # total_norm = √(∑ ||g_i||²)
  # if total_norm > max_norm:
  #   scale = max_norm / total_norm
  #   g_i = g_i * scale

Typical values:
  - max_norm = 1.0 (standard)
  - max_norm = 0.5 (more conservative)
  - max_norm = 5.0 (less aggressive)

When to apply:
  After backward(), before optimizer.step()

Full training step with clipping:
  optimizer.zero_grad()

  loss = train_step(model, batch)
  loss.backward()

  # Clip gradients
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

  optimizer.step()
  scheduler.step()
```

**19.3.2 Monitoring Gradient Norms**

```
Track gradient health:

def compute_gradient_norm(model):
  """Compute global gradient norm."""
  total_norm = 0.0
  for p in model.parameters():
    if p.grad is not None:
      param_norm = p.grad.data.norm(2)
      total_norm += param_norm.item() ** 2
  total_norm = total_norm ** 0.5
  return total_norm

# In training loop:
loss.backward()

grad_norm = compute_gradient_norm(model)
print(f"Gradient norm: {grad_norm:.4f}")

torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

Healthy gradient norms:
  - 0.1 - 10.0: Normal
  - < 0.01: Vanishing gradients (increase LR)
  - > 100: Exploding gradients (decrease LR or clip more)
```

#### **19.4 Optimizer Configuration Class**

**19.4.1 OptimizerConfig**

```
@dataclass
class OptimizerConfig:
  # Optimizer type
  optimizer_type: str = "adamw"

  # Learning rates
  base_lr: float = 1e-4
  vision_lr: float = 1e-5
  projection_lr: float = 5e-4
  decoder_lr: float = 1e-4
  classifier_lr: float = 5e-4

  # AdamW parameters
  betas: Tuple[float, float] = (0.9, 0.999)
  eps: float = 1e-8
  weight_decay: float = 0.01

  # Scheduler
  scheduler_type: str = "cosine_warmup"
  warmup_ratio: float = 0.1  # 10% of training
  min_lr_ratio: float = 0.1  # End at 10% of base LR

  # Gradient clipping
  max_grad_norm: float = 1.0

  # Mixed precision
  use_amp: bool = True

  # Differential LR
  use_differential_lr: bool = False

  def __post_init__(self):
    if self.scheduler_type not in ["cosine_warmup", "linear", "plateau", "constant"]:
      raise ValueError(f"Unknown scheduler type: {self.scheduler_type}")
```

**19.4.2 Optimizer Factory**

```
def create_optimizer_and_scheduler(model, train_dataloader, config):
  """
  Create optimizer and scheduler.

  Args:
    model: TrafficVLM model
    train_dataloader: Training data loader
    config: OptimizerConfig

  Returns:
    optimizer, scheduler
  """

  # Calculate total steps
  num_epochs = config.num_epochs
  steps_per_epoch = len(train_dataloader)
  total_steps = num_epochs * steps_per_epoch
  warmup_steps = int(config.warmup_ratio * total_steps)

  # Get parameter groups
  if config.use_differential_lr:
    param_groups = [
      {"params": model.vision_encoder.parameters(), "lr": config.vision_lr},
      {"params": model.vision_projection.parameters(), "lr": config.projection_lr},
      {"params": model.decoder.parameters(), "lr": config.decoder_lr},
      {"params": model.classifier.parameters(), "lr": config.classifier_lr}
    ]
  else:
    param_groups = get_parameter_groups(model, config.weight_decay)

  # Create optimizer
  if config.optimizer_type == "adamw":
    optimizer = AdamW(
      param_groups,
      lr=config.base_lr if not config.use_differential_lr else 1.0,  # LR in groups
      betas=config.betas,
      eps=config.eps,
      weight_decay=config.weight_decay
    )
  else:
    raise ValueError(f"Unknown optimizer: {config.optimizer_type}")

  # Create scheduler
  if config.scheduler_type == "cosine_warmup":
    scheduler = get_cosine_schedule_with_warmup(
      optimizer,
      num_warmup_steps=warmup_steps,
      num_training_steps=total_steps,
      min_lr_ratio=config.min_lr_ratio
    )
  elif config.scheduler_type == "linear":
    scheduler = LinearLR(optimizer, start_factor=0.01, total_iters=warmup_steps)
  elif config.scheduler_type == "plateau":
    scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3)
  elif config.scheduler_type == "constant":
    scheduler = None
  else:
    raise ValueError(f"Unknown scheduler: {config.scheduler_type}")

  print(f"Optimizer created:")
  print(f"  Type: {config.optimizer_type}")
  print(f"  Base LR: {config.base_lr}")
  print(f"  Weight Decay: {config.weight_decay}")
  print(f"  Scheduler: {config.scheduler_type}")
  print(f"  Total Steps: {total_steps}")
  print(f"  Warmup Steps: {warmup_steps}")

  return optimizer, scheduler
```

#### **19.5 Testing Optimization**

**19.5.1 Optimizer Tests**

```
1. test_optimizer_creation():
   config = OptimizerConfig(base_lr=1e-4)
   model = TrafficVLM(vlm_config)

   optimizer, scheduler = create_optimizer_and_scheduler(
     model, train_dataloader, config
   )

   assert optimizer is not None
   assert len(optimizer.param_groups) > 0

2. test_learning_rate_schedule():
   optimizer, scheduler = create_optimizer_and_scheduler(...)

   lrs = []
   for step in range(1000):
     lrs.append(optimizer.param_groups[0]['lr'])
     scheduler.step()

   # Check warmup
   assert lrs[100] > lrs[0]  # Increasing

   # Check decay
   assert lrs[-1] < lrs[500]  # Decreasing

3. test_gradient_clipping():
   model = TrafficVLM(config)
   optimizer = AdamW(model.parameters(), lr=1e-4)

   # Create large gradients
   for p in model.parameters():
     if p.requires_grad:
       p.grad = torch.randn_like(p) * 100  # Large gradients

   # Clip
   norm_before = compute_gradient_norm(model)
   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
   norm_after = compute_gradient_norm(model)

   assert norm_before > 10.0
   assert norm_after <= 1.0

4. test_weight_decay():
   # Model with weight decay
   model1 = TrafficVLM(config)
   opt1 = AdamW(model1.parameters(), lr=1e-4, weight_decay=0.01)

   # Model without weight decay
   model2 = TrafficVLM(config)
   model2.load_state_dict(model1.state_dict())
   opt2 = AdamW(model2.parameters(), lr=1e-4, weight_decay=0.0)

   # Train for a few steps
   for _ in range(10):
     # Same batch
     loss1 = train_step(model1, batch)
     loss1.backward()
     opt1.step()
     opt1.zero_grad()

     loss2 = train_step(model2, batch)
     loss2.backward()
     opt2.step()
     opt2.zero_grad()

   # Parameters should differ (weight decay regularizes)
   param_diff = sum((p1 - p2).abs().sum() for p1, p2 in
                    zip(model1.parameters(), model2.parameters()))
   assert param_diff > 0
```

---

### **Task 20: train.py - Training Loop**

#### **20.1 Training Loop Structure**

**20.1.1 Main Training Function**

```
def train(
  model,
  train_dataloader,
  val_dataloader,
  optimizer,
  scheduler,
  config,
  device="cuda"
):
  """
  Main training loop.

  Args:
    model: TrafficVLM instance
    train_dataloader: Training data
    val_dataloader: Validation data
    optimizer: Optimizer instance
    scheduler: LR scheduler
    config: TrainingConfig
    device: Device to train on

  Returns:
    training_history: Dict of metrics
  """

  model.to(device)

  # Mixed precision training
  if config.use_amp:
    from torch.cuda.amp import autocast, GradScaler
    scaler = GradScaler()
  else:
    scaler = None

  # Training history
  history = {
    "train_loss": [],
    "train_accuracy": [],
    "val_loss": [],
    "val_accuracy": [],
    "learning_rates": []
  }

  # Best model tracking
  best_val_accuracy = 0.0
  patience_counter = 0

  # Training loop
  for epoch in range(config.num_epochs):
    print(f"\n{'='*80}")
    print(f"Epoch {epoch+1}/{config.num_epochs}")
    print(f"{'='*80}")

    # Train one epoch
    train_metrics = train_epoch(
      model,
      train_dataloader,
      optimizer,
      scheduler,
      scaler,
      config,
      device,
      epoch
    )

    # Validate
    val_metrics = validate(model, val_dataloader, device)

    # Log metrics
    history["train_loss"].append(train_metrics["loss"])
    history["train_accuracy"].append(train_metrics["accuracy"])
    history["val_loss"].append(val_metrics["loss"])
    history["val_accuracy"].append(val_metrics["accuracy"])
    history["learning_rates"].append(optimizer.param_groups[0]["lr"])

    # Print epoch summary
    print(f"\nEpoch {epoch+1} Summary:")
    print(f"  Train Loss: {train_metrics['loss']:.4f}")
    print(f"  Train Acc:  {train_metrics['accuracy']:.4f}")
    print(f"  Val Loss:   {val_metrics['loss']:.4f}")
    print(f"  Val Acc:    {val_metrics['accuracy']:.4f}")
    print(f"  LR:         {optimizer.param_groups[0]['lr']:.2e}")

    # Save checkpoint
    if val_metrics["accuracy"] > best_val_accuracy:
      best_val_accuracy = val_metrics["accuracy"]
      patience_counter = 0

      save_checkpoint(
        model,
        optimizer,
        scheduler,
        epoch,
        val_metrics,
        config.checkpoint_dir / f"best_model.pt"
      )
      print(f"  ✓ New best model saved! (Val Acc: {best_val_accuracy:.4f})")
    else:
      patience_counter += 1

    # Early stopping
    if config.early_stopping and patience_counter >= config.patience:
      print(f"\nEarly stopping triggered after {epoch+1} epochs")
      break

    # Regular checkpoint
    if (epoch + 1) % config.save_every == 0:
      save_checkpoint(
        model,
        optimizer,
        scheduler,
        epoch,
        val_metrics,
        config.checkpoint_dir / f"checkpoint_epoch_{epoch+1}.pt"
      )

  print(f"\n{'='*80}")
  print(f"Training completed!")
  print(f"Best validation accuracy: {best_val_accuracy:.4f}")
  print(f"{'='*80}")

  return history
```

**20.1.2 Single Epoch Training**

```
def train_epoch(
  model,
  dataloader,
  optimizer,
  scheduler,
  scaler,
  config,
  device,
  epoch
):
  """Train for one epoch."""

  model.train()

  total_loss = 0.0
  total_correct = 0
  total_samples = 0

  # Progress bar
  from tqdm import tqdm
  pbar = tqdm(dataloader, desc=f"Training Epoch {epoch+1}")

  for batch_idx, batch in enumerate(pbar):
    # Move batch to device
    images = batch["images"].to(device)
    input_ids = batch["input_ids"].to(device)
    attention_mask = batch["attention_mask"].to(device)
    labels = batch["labels"].to(device)

    # Forward pass
    if config.use_amp:
      with autocast():
        outputs = model(
          pixel_values=images,
          input_ids=input_ids,
          attention_mask=attention_mask,
          labels=labels
        )
        loss = outputs["loss"]
    else:
      outputs = model(
        pixel_values=images,
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=labels
      )
      loss = outputs["loss"]

    # Gradient accumulation
    if config.gradient_accumulation_steps > 1:
      loss = loss / config.gradient_accumulation_steps

    # Backward pass
    if config.use_amp:
      scaler.scale(loss).backward()
    else:
      loss.backward()

    # Optimizer step (with accumulation)
    if (batch_idx + 1) % config.gradient_accumulation_steps == 0:
      # Gradient clipping
      if config.max_grad_norm > 0:
        if config.use_amp:
          scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)

      # Optimizer step
      if config.use_amp:
        scaler.step(optimizer)
        scaler.update()
      else:
        optimizer.step()

      # Scheduler step
      if scheduler is not None and config.scheduler_step_per_batch:
        scheduler.step()

      optimizer.zero_grad()

    # Metrics
    logits = outputs["logits"]
    predictions = torch.argmax(logits, dim=-1)
    correct = (predictions == labels).sum().item()

    batch_size = images.size(0)
    total_loss += loss.item() * batch_size * config.gradient_accumulation_steps
    total_correct += correct
    total_samples += batch_size

    # Update progress bar
    pbar.set_postfix({
      "loss": f"{loss.item():.4f}",
      "acc": f"{correct/batch_size:.4f}"
    })

  # Epoch metrics
  avg_loss = total_loss / total_samples
  accuracy = total_correct / total_samples

  return {
    "loss": avg_loss,
    "accuracy": accuracy
  }
```

#### **20.2 Training Configuration**

**20.2.1 TrainingConfig Class**

```
@dataclass
class TrainingConfig:
  # Training hyperparameters
  num_epochs: int = 20
  batch_size: int = 4
  gradient_accumulation_steps: int = 4  # Effective batch = 4×4 = 16

  # Optimization
  learning_rate: float = 1e-4
  weight_decay: float = 0.01
  max_grad_norm: float = 1.0

  # Scheduler
  warmup_ratio: float = 0.1
  scheduler_step_per_batch: bool = True

  # Mixed precision
  use_amp: bool = True

  # Early stopping
  early_stopping: bool = True
  patience: int = 5

  # Checkpointing
  checkpoint_dir: Path = Path("checkpoints")
  save_every: int = 5  # Save every N epochs

  # Logging
  log_every: int = 100  # Log every N steps
  eval_every: int = 1  # Evaluate every N epochs

  # Device
  device: str = "cuda" if torch.cuda.is_available() else "cpu"

  # Reproducibility
  seed: int = 42

  # Data
  num_workers: int = 2
  pin_memory: bool = True

  def __post_init__(self):
    self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
```

#### **20.3 Checkpointing**

**20.3.1 Save Checkpoint**

```
def save_checkpoint(
  model,
  optimizer,
  scheduler,
  epoch,
  metrics,
  path
):
  """
  Save training checkpoint.

  Args:
    model: Model to save
    optimizer: Optimizer state
    scheduler: Scheduler state
    epoch: Current epoch
    metrics: Validation metrics
    path: Save path
  """
  checkpoint = {
    "epoch": epoch,
    "model_state_dict": model.state_dict(),
    "optimizer_state_dict": optimizer.state_dict(),
    "scheduler_state_dict": scheduler.state_dict() if scheduler else None,
    "metrics": metrics,
    "config": model.config,
    "timestamp": datetime.now().isoformat()
  }

  torch.save(checkpoint, path)
  print(f"Checkpoint saved to {path}")
```

**20.3.2 Load Checkpoint**

```
def load_checkpoint(
  model,
  optimizer,
  scheduler,
  path,
  device="cuda"
):
  """
  Load training checkpoint.

  Returns:
    start_epoch: Epoch to resume from
    metrics: Validation metrics
  """
  checkpoint = torch.load(path, map_location=device)

  model.load_state_dict(checkpoint["model_state_dict"])
  optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

  if scheduler and checkpoint["scheduler_state_dict"]:
    scheduler.load_state_dict(checkpoint["scheduler_state_dict"])

  start_epoch = checkpoint["epoch"] + 1
  metrics = checkpoint["metrics"]

  print(f"Checkpoint loaded from {path}")
  print(f"Resuming from epoch {start_epoch}")

  return start_epoch, metrics
```

**20.3.3 Resume Training**

```
def resume_training(checkpoint_path, train_dataloader, val_dataloader, config):
  """Resume training from checkpoint."""

  # Load checkpoint
  checkpoint = torch.load(checkpoint_path)

  # Recreate model
  model = TrafficVLM(checkpoint["config"])
  model.load_state_dict(checkpoint["model_state_dict"])

  # Recreate optimizer
  optimizer, scheduler = create_optimizer_and_scheduler(
    model, train_dataloader, config
  )
  optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

  if scheduler and checkpoint["scheduler_state_dict"]:
    scheduler.load_state_dict(checkpoint["scheduler_state_dict"])

  start_epoch = checkpoint["epoch"] + 1

  # Continue training
  for epoch in range(start_epoch, config.num_epochs):
    train_metrics = train_epoch(...)
    val_metrics = validate(...)
    ...
```

---

Will continue with Tasks 21-25 (Evaluation, Logging, Validation, Hyperparameter Tuning) in the next response. Ready to proceed?
