# Traffic Scene VLM - ULTRA-DETAILED Tasks 11-17

---

## **PHASE 4: LANGUAGE DECODER (Day 4)**

---

### **Task 11: gemma_decoder.py - Main Language Decoder**

#### **11.1 Gemma Decoder Architecture Overview**

**11.1.1 Architecture Design Philosophy**

```
Gemma Decoder follows PaliGemma paper design:

Key Characteristics:
1. Decoder-only architecture (like GPT)
2. Causal self-attention (can't see future tokens)
3. Cross-attention to vision tokens
4. Grouped Query Attention (GQA) for efficiency
5. RMS Normalization (not LayerNorm)
6. Rotary Position Embeddings (RoPE)
7. SwiGLU activation (instead of GELU)
8. KV-Cache for efficient generation

Why Decoder-only?
  - Flexible for both classification and generation
  - Can autoregressively generate answers
  - Better for few-shot learning
  - Follows modern LLM trends (GPT, LLaMA, Gemma)

Architecture Flow:
Input command tokens → Text embeddings → Decoder layers → Output logits
                           ↑
                    Vision tokens (cross-attention)
```

**11.1.2 Model Size Configuration**

```
GemmaDecoderConfig (Optimized for A3000):

Small Configuration:
  vocab_size: 500                    # Traffic domain vocab
  hidden_size: 512                   # Smaller than vision (768)
  intermediate_size: 2048            # 4x hidden (SwiGLU)
  num_hidden_layers: 4               # Fewer layers for speed
  num_attention_heads: 8             # Multi-head attention
  num_key_value_heads: 2             # GQA 4:1 ratio
  head_dim: 64                       # 512 / 8 = 64
  max_position_embeddings: 128       # Max command length
  rms_norm_eps: 1e-6                 # RMS norm epsilon
  rope_theta: 10000.0                # RoPE base frequency
  attention_dropout: 0.0             # Usually 0 for decoders
  hidden_dropout: 0.1                # Regular dropout
  use_cache: True                    # KV-cache for inference

Parameter Count Estimation:
  Text embeddings: 500 × 512 = 256K params

  Per decoder layer:
    - Self-attention: ~1.5M params (with GQA)
    - Cross-attention: ~1.5M params
    - FFN: 2 × 512 × 2048 = 2.1M params
    - Total per layer: ~5M params

  4 layers × 5M = 20M params
  Output head: 512 × 500 = 256K params (weight tied)

  Total Decoder: ~20M params ✓

Combined with Vision Encoder (22M):
  Total VLM: ~42M params ✓ (fits in 6GB VRAM)

Medium Configuration (if more VRAM):
  hidden_size: 768
  num_hidden_layers: 6
  Total: ~50M params
```

**11.1.3 Decoder Class Structure**

```
Class: GemmaDecoder(nn.Module)

Attributes:
  - config: GemmaDecoderConfig
  - vocab_size: int
  - embed_tokens: nn.Embedding          # Word embeddings
  - layers: nn.ModuleList               # Decoder layers
  - norm: RMSNorm                       # Final normalization
  - gradient_checkpointing: bool

Methods:
  1. __init__(config)
  2. get_input_embeddings() → nn.Embedding
  3. set_input_embeddings(embeddings)
  4. forward(
       input_ids,
       attention_mask=None,
       encoder_hidden_states=None,  # Vision tokens
       encoder_attention_mask=None,
       past_key_values=None,        # KV-cache
       use_cache=False,
       output_attentions=False,
       output_hidden_states=False
     ) → dict
  5. _prepare_decoder_attention_mask(...)
```

#### **11.2 Text Embeddings**

**11.2.1 Token Embedding Implementation**

```
Token Embedding Layer:

Purpose: Convert token IDs to dense vectors

Implementation:
  self.embed_tokens = nn.Embedding(
    num_embeddings=config.vocab_size,  # 500
    embedding_dim=config.hidden_size,  # 512
    padding_idx=0                      # [PAD] token ID
  )

Padding Handling:
  - padding_idx=0 ensures [PAD] embeddings don't contribute to gradients
  - Padding embeddings remain zero throughout training

Initialization:
  nn.init.normal_(self.embed_tokens.weight, mean=0.0, std=0.02)

  # Explicitly zero out padding embedding
  with torch.no_grad():
    self.embed_tokens.weight[0].zero_()

Shape Transformation:
  Input: input_ids [B, seq_len] (e.g., [4, 20])
  Output: embeddings [B, seq_len, hidden_size] (e.g., [4, 20, 512])

Example:
  input_ids = torch.tensor([[2, 45, 67, 12, 3, 0, 0]])  # [1, 7]
  embeddings = self.embed_tokens(input_ids)  # [1, 7, 512]

  # embeddings[0, 0] = embedding for [SOS]
  # embeddings[0, 5] = zero vector (padding)
```

**11.2.2 Embedding Scaling (Optional)**

```
Some models scale embeddings by sqrt(hidden_size)

Why?
  - Prevents embeddings from dominating early layers
  - Balances contribution with position embeddings
  - Not standard in Gemma, but common in other models

Implementation (if using):
  embeddings = self.embed_tokens(input_ids)
  embeddings = embeddings * math.sqrt(self.config.hidden_size)

For this project: Don't scale (follow Gemma design)
```

#### **11.3 Decoder Forward Pass**

**11.3.1 Complete Forward Method**

```
def forward(
  self,
  input_ids: torch.LongTensor,                    # [B, seq_len]
  attention_mask: Optional[torch.Tensor] = None,  # [B, seq_len]
  encoder_hidden_states: Optional[torch.Tensor] = None,  # Vision tokens [B, 196, 768]
  encoder_attention_mask: Optional[torch.Tensor] = None, # [B, 196]
  past_key_values: Optional[List[Tuple]] = None,  # KV-cache
  use_cache: bool = False,
  output_attentions: bool = False,
  output_hidden_states: bool = False,
  return_dict: bool = True
) -> dict:
  """
  Full decoder forward pass with cross-attention to vision.

  Args:
    input_ids: Token IDs [B, seq_len]
    attention_mask: Padding mask [B, seq_len]
    encoder_hidden_states: Vision tokens from encoder [B, 196, vision_dim]
    encoder_attention_mask: Mask for vision tokens
    past_key_values: Cached K,V from previous generation steps
    use_cache: Whether to return KV-cache for next step
    output_attentions: Return attention weights
    output_hidden_states: Return all layer hidden states
    return_dict: Return dict (True) or tuple (False)

  Returns:
    dict: {
      "last_hidden_state": [B, seq_len, hidden_size],
      "past_key_values": List of cached K,V,
      "hidden_states": List of hidden states,
      "attentions": List of attention weights
    }
  """

  batch_size, seq_length = input_ids.shape

  # Step 1: Get input embeddings
  hidden_states = self.embed_tokens(input_ids)
  # Shape: [B, seq_len, 512]

  # Step 2: Prepare causal attention mask
  # Decoder must not see future tokens
  if attention_mask is None:
    attention_mask = torch.ones((batch_size, seq_length), device=input_ids.device)

  causal_mask = self._prepare_decoder_attention_mask(
    attention_mask,
    input_shape=(batch_size, seq_length),
    past_key_values_length=0 if past_key_values is None else past_key_values[0][0].shape[2]
  )
  # Shape: [B, 1, seq_len, seq_len + past_len]

  # Step 3: Prepare encoder attention mask (for cross-attention)
  if encoder_hidden_states is not None:
    if encoder_attention_mask is None:
      encoder_length = encoder_hidden_states.shape[1]
      encoder_attention_mask = torch.ones(
        (batch_size, encoder_length),
        device=input_ids.device
      )

    # Expand encoder mask [B, 196] -> [B, 1, 1, 196]
    encoder_attention_mask = encoder_attention_mask[:, None, None, :]
    encoder_attention_mask = (1.0 - encoder_attention_mask) * torch.finfo(hidden_states.dtype).min

  # Step 4: Initialize outputs
  all_hidden_states = () if output_hidden_states else None
  all_self_attentions = () if output_attentions else None
  all_cross_attentions = () if output_attentions else None
  next_decoder_cache = () if use_cache else None

  # Step 5: Pass through decoder layers
  for idx, decoder_layer in enumerate(self.layers):

    if output_hidden_states:
      all_hidden_states += (hidden_states,)

    # Get past key-value for this layer (if using cache)
    past_key_value = past_key_values[idx] if past_key_values is not None else None

    # Layer forward pass
    layer_outputs = decoder_layer(
      hidden_states,
      attention_mask=causal_mask,
      encoder_hidden_states=encoder_hidden_states,
      encoder_attention_mask=encoder_attention_mask,
      past_key_value=past_key_value,
      output_attentions=output_attentions,
      use_cache=use_cache
    )

    # Update hidden states
    hidden_states = layer_outputs[0]

    # Cache for next generation step
    if use_cache:
      next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)

    # Collect attention weights
    if output_attentions:
      all_self_attentions += (layer_outputs[1],)
      if encoder_hidden_states is not None:
        all_cross_attentions += (layer_outputs[2],)

  # Step 6: Final layer normalization
  hidden_states = self.norm(hidden_states)
  # Shape: [B, seq_len, 512]

  # Step 7: Add final hidden state
  if output_hidden_states:
    all_hidden_states += (hidden_states,)

  # Step 8: Prepare output
  if not return_dict:
    return tuple(v for v in [
      hidden_states,
      next_decoder_cache,
      all_hidden_states,
      all_self_attentions,
      all_cross_attentions
    ] if v is not None)

  return {
    "last_hidden_state": hidden_states,
    "past_key_values": next_decoder_cache,
    "hidden_states": all_hidden_states,
    "attentions": all_self_attentions,
    "cross_attentions": all_cross_attentions
  }

Key Points:
  - Causal mask prevents attending to future tokens
  - Cross-attention to vision tokens happens in each layer
  - KV-cache stored for efficient generation
  - RoPE applied inside decoder layers (not here)
```

**11.3.2 Causal Attention Mask Creation**

```
def _prepare_decoder_attention_mask(
  self,
  attention_mask,      # [B, seq_len]
  input_shape,         # (batch_size, seq_len)
  past_key_values_length=0
):
  """
  Create causal mask combined with padding mask.

  Causal mask ensures token i can only attend to tokens 0..i

  Returns:
    mask: [B, 1, seq_len, seq_len + past_len]
  """
  batch_size, seq_length = input_shape

  # Create causal mask (lower triangular)
  # Shape: [seq_len, seq_len]
  causal_mask = torch.triu(
    torch.ones((seq_length, seq_length), dtype=torch.bool),
    diagonal=1
  )
  # Example for seq_len=4:
  # [[0, 1, 1, 1],    0 = can attend, 1 = cannot attend
  #  [0, 0, 1, 1],
  #  [0, 0, 0, 1],
  #  [0, 0, 0, 0]]

  # Invert (we want 0 = cannot attend for masking)
  causal_mask = ~causal_mask

  # If using KV-cache, extend mask
  if past_key_values_length > 0:
    # Can attend to all past tokens
    causal_mask = torch.cat([
      torch.ones((seq_length, past_key_values_length), dtype=torch.bool),
      causal_mask
    ], dim=-1)

  # Add batch and head dimensions
  causal_mask = causal_mask[None, None, :, :]  # [1, 1, seq_len, total_len]
  causal_mask = causal_mask.expand(batch_size, 1, seq_length, -1)

  # Combine with padding mask
  if attention_mask is not None:
    # attention_mask: [B, seq_len]
    # Expand to [B, 1, 1, seq_len]
    expanded_mask = attention_mask[:, None, None, :]

    # Combine: can attend only if both masks allow
    combined_mask = causal_mask & expanded_mask
  else:
    combined_mask = causal_mask

  # Convert boolean to additive mask
  # True → 0.0 (can attend)
  # False → -inf (cannot attend)
  combined_mask = combined_mask.to(dtype=torch.float32)
  combined_mask = (1.0 - combined_mask) * torch.finfo(torch.float32).min

  return combined_mask

Visual Example of Combined Mask (seq_len=5, token 3 is padding):

  Padding mask:     Causal mask:      Combined:
  [1, 1, 1, 0, 1]   [1, 0, 0, 0, 0]   [1, 0, 0, 0, 0]
                    [1, 1, 0, 0, 0]   [1, 1, 0, 0, 0]
                    [1, 1, 1, 0, 0]   [1, 1, 1, 0, 0]
                    [1, 1, 1, 1, 0]   [0, 0, 0, 0, 0]  (padding token)
                    [1, 1, 1, 0, 1]   [1, 1, 1, 0, 1]

  Token 0 can attend to: [0]
  Token 1 can attend to: [0, 1]
  Token 2 can attend to: [0, 1, 2]
  Token 3 (padding) cannot attend to anyone
  Token 4 can attend to: [0, 1, 2, 4] (not 3, it's padding)
```

#### **11.4 Output Head (Language Model Head)**

**11.4.1 LM Head Implementation**

```
Class: GemmaForCausalLM (wraps decoder)

Purpose: Add output projection to vocabulary

Structure:
  class GemmaForCausalLM(nn.Module):
    def __init__(self, config):
      super().__init__()

      self.decoder = GemmaDecoder(config)

      # LM head: project hidden states to vocab logits
      self.lm_head = nn.Linear(
        config.hidden_size,   # 512
        config.vocab_size,    # 500
        bias=False            # No bias (standard for LM heads)
      )

      # Weight tying (optional but recommended)
      if config.tie_word_embeddings:
        self.lm_head.weight = self.decoder.embed_tokens.weight

    def forward(self, input_ids, **kwargs):
      # Decoder forward
      decoder_outputs = self.decoder(input_ids, **kwargs)

      # Get last hidden state [B, seq_len, 512]
      hidden_states = decoder_outputs["last_hidden_state"]

      # Project to vocabulary [B, seq_len, 500]
      logits = self.lm_head(hidden_states)

      return {
        "logits": logits,
        **decoder_outputs
      }

Weight Tying:
  - Share weights between input embeddings and output projection
  - Reduces parameters: 500 × 512 = 256K saved
  - Improves performance (empirically shown)
  - Standard in modern LLMs

If NOT tying:
  - Initialize lm_head with same distribution as embeddings
  nn.init.normal_(self.lm_head.weight, mean=0.0, std=0.02)
```

**11.4.2 Output Processing for Classification**

```
For YES/NO classification task:

Option 1: Use specific token logits
  # Get logits for [YES] and [NO] tokens
  yes_token_id = tokenizer.vocab["[YES]"]  # e.g., 4
  no_token_id = tokenizer.vocab["[NO]"]    # e.g., 5

  # Get logits [B, seq_len, 500]
  logits = lm_head(hidden_states)

  # Take last token position (prediction)
  last_logits = logits[:, -1, :]  # [B, 500]

  # Extract YES/NO logits
  classification_logits = torch.stack([
    last_logits[:, no_token_id],   # NO score
    last_logits[:, yes_token_id]   # YES score
  ], dim=-1)  # [B, 2]

  # Apply softmax
  probs = F.softmax(classification_logits, dim=-1)

  # Predict
  predictions = torch.argmax(probs, dim=-1)  # 0=NO, 1=YES

Option 2: Separate classification head
  # Instead of lm_head, use classification head
  self.classifier = nn.Linear(config.hidden_size, 2)  # Binary

  # Use CLS token or last token representation
  last_hidden = hidden_states[:, -1, :]  # [B, 512]
  logits = self.classifier(last_hidden)  # [B, 2]

For this project: Use Option 1 (token-based)
  - More aligned with language model design
  - Allows text generation if needed later
  - [YES] and [NO] tokens naturally represent classes
```

#### **11.5 KV-Cache Implementation**

**11.5.1 Why KV-Cache?**

```
Problem: Autoregressive generation is slow

Without cache:
  Step 1: Generate token 1
    - Process: [SOS]
    - Output: token_1

  Step 2: Generate token 2
    - Process: [SOS, token_1]  ← Recomputes attention for SOS!
    - Output: token_2

  Step 3: Generate token 3
    - Process: [SOS, token_1, token_2]  ← Recomputes everything!
    - Output: token_3

  Time complexity: O(n²) where n is sequence length

With KV-cache:
  Step 1: Generate token 1
    - Process: [SOS]
    - Compute K, V for SOS
    - Cache them
    - Output: token_1

  Step 2: Generate token 2
    - Process: [token_1] only!
    - Compute K, V for token_1
    - Concatenate with cached K, V
    - Output: token_2

  Step 3: Generate token 3
    - Process: [token_2] only!
    - Compute K, V for token_2
    - Concatenate with cached K, V
    - Output: token_3

  Time complexity: O(n)

Speedup: 5-10x faster generation
```

**11.5.2 Cache Data Structure**

```
KV-Cache Structure:

For each decoder layer, cache:
  - Self-attention K, V
  - Cross-attention K, V (if encoder present)

Type: Tuple of tuples

Structure:
  past_key_values = (
    # Layer 0
    (
      self_attn_key,    # [B, num_heads, past_len, head_dim]
      self_attn_value,  # [B, num_heads, past_len, head_dim]
      cross_attn_key,   # [B, num_heads, encoder_len, head_dim]
      cross_attn_value  # [B, num_heads, encoder_len, head_dim]
    ),
    # Layer 1
    (...),
    # Layer 2
    (...),
    # Layer 3
    (...)
  )

Memory Footprint:
  For 4 layers, 8 heads, head_dim=64:

  Self-attention KV per layer:
    2 × [B, 8, seq_len, 64]
    = B × 8 × seq_len × 64 × 2 × 4 bytes (FP32)
    = B × 4KB × seq_len

  For batch=1, seq_len=50: 200KB per layer
  4 layers: 800KB (negligible)

  Cross-attention KV per layer:
    2 × [B, 8, 196, 64]
    = B × 100KB per layer
  4 layers: 400KB

  Total cache: ~1.2MB (tiny!)

Conclusion: KV-cache is very memory-efficient
```

**11.5.3 Using Cache in Forward Pass**

```
During generation:

# First step (prefill)
outputs = decoder(
  input_ids=torch.tensor([[2, 45, 67]]),  # [SOS] + initial tokens
  encoder_hidden_states=vision_tokens,
  use_cache=True
)

hidden_states = outputs["last_hidden_state"]  # [1, 3, 512]
past_key_values = outputs["past_key_values"]  # Cached K, V

# Generate next token
logits = lm_head(hidden_states[:, -1:, :])  # Last position
next_token = torch.argmax(logits, dim=-1)

# Second step (decode with cache)
outputs = decoder(
  input_ids=next_token.unsqueeze(0),  # [1, 1] - only new token
  encoder_hidden_states=vision_tokens,
  past_key_values=past_key_values,    # Reuse cache
  use_cache=True
)

# past_key_values updated with new K, V
past_key_values = outputs["past_key_values"]

# Continue until [EOS] or max_length
```

#### **11.6 Gradient Checkpointing**

**11.6.1 Memory-Compute Tradeoff**

```
Problem: Deep models use lots of memory for activations

Solution: Gradient checkpointing
  - Don't store intermediate activations during forward
  - Recompute them during backward pass
  - Trade compute for memory (2x slower, 1/N memory)

Implementation:
  class GemmaDecoder(nn.Module):
    def __init__(self, config):
      super().__init__()
      self.gradient_checkpointing = False
      ...

    def forward(self, ...):
      for idx, decoder_layer in enumerate(self.layers):

        if self.gradient_checkpointing and self.training:
          # Use checkpointing
          def create_custom_forward(module):
            def custom_forward(*inputs):
              return module(*inputs, output_attentions=False)
            return custom_forward

          layer_outputs = torch.utils.checkpoint.checkpoint(
            create_custom_forward(decoder_layer),
            hidden_states,
            causal_mask,
            encoder_hidden_states,
            ...
          )
        else:
          # Normal forward
          layer_outputs = decoder_layer(...)

Enable during training:
  model.decoder.gradient_checkpointing = True

When to use:
  - If getting OOM errors during training
  - If want to use larger batch size
  - Trade-off: ~2x slower training, but fits in memory
```

#### **11.7 Testing & Validation**

**11.7.1 Decoder Tests**

```
1. test_decoder_forward():
   config = GemmaDecoderConfig(vocab_size=500, hidden_size=512)
   decoder = GemmaDecoder(config)

   input_ids = torch.randint(0, 500, (2, 10))
   outputs = decoder(input_ids)

   assert outputs["last_hidden_state"].shape == (2, 10, 512)

2. test_decoder_with_vision():
   vision_tokens = torch.randn(2, 196, 768)
   outputs = decoder(
     input_ids,
     encoder_hidden_states=vision_tokens
   )
   assert outputs["last_hidden_state"].shape == (2, 10, 512)

3. test_causal_masking():
   # Token at position i should not affect tokens at positions < i
   input_ids = torch.randint(0, 500, (1, 5))

   # Forward pass
   outputs1 = decoder(input_ids)
   hidden1 = outputs1["last_hidden_state"]

   # Change last token
   input_ids_modified = input_ids.clone()
   input_ids_modified[0, -1] = 100

   outputs2 = decoder(input_ids_modified)
   hidden2 = outputs2["last_hidden_state"]

   # First 4 tokens should have same hidden states (due to causal mask)
   assert torch.allclose(hidden1[0, :4], hidden2[0, :4], atol=1e-5)

   # Last token should differ
   assert not torch.allclose(hidden1[0, 4], hidden2[0, 4])

4. test_kv_cache_consistency():
   input_ids = torch.randint(0, 500, (1, 5))

   # Without cache
   outputs_no_cache = decoder(input_ids, use_cache=False)
   hidden_no_cache = outputs_no_cache["last_hidden_state"]

   # With cache (2 steps)
   outputs1 = decoder(input_ids[:, :3], use_cache=True)
   past_kv = outputs1["past_key_values"]

   outputs2 = decoder(
     input_ids[:, 3:],
     past_key_values=past_kv,
     use_cache=True
   )
   hidden_with_cache = outputs2["last_hidden_state"]

   # Last hidden state should match
   assert torch.allclose(
     hidden_no_cache[0, -1],
     hidden_with_cache[0, -1],
     atol=1e-4
   )

5. test_attention_mask():
   input_ids = torch.tensor([[2, 45, 67, 0, 0]])  # 2 padding tokens
   attention_mask = torch.tensor([[1, 1, 1, 0, 0]])

   outputs = decoder(input_ids, attention_mask=attention_mask)

   # Should not crash and produce valid output
   assert outputs["last_hidden_state"].shape == (1, 5, 512)
```

---

### **Task 12: decoder_layer.py - Single Decoder Block**

#### **12.1 Decoder Layer Architecture**

**12.1.1 Layer Components**

```
GemmaDecoderLayer consists of:

1. Input Layer Norm (RMSNorm)
2. Self-Attention (with causal mask)
3. Residual Connection
4. Post-Attention Layer Norm (RMSNorm)
5. Cross-Attention (to vision tokens)
6. Residual Connection
7. Post-Cross-Attention Layer Norm (RMSNorm)
8. Feed-Forward Network (SwiGLU)
9. Residual Connection

Pre-Normalization Architecture:
  x → Norm → Attention → Add(x, ·)
  x → Norm → FFN → Add(x, ·)

Benefits:
  - Stable gradients in deep networks
  - No gradient explosion
  - Standard in modern transformers
```

**12.1.2 Decoder Layer Class**

```
Class: GemmaDecoderLayer(nn.Module)

Attributes:
  - hidden_size: int
  - self_attn: GemmaSelfAttention (causal, with GQA)
  - cross_attn: GemmaCrossAttention (to vision)
  - mlp: GemmaFeedForward (SwiGLU)
  - input_layernorm: RMSNorm
  - post_attention_layernorm: RMSNorm
  - post_cross_attention_layernorm: RMSNorm

Methods:
  - __init__(config)
  - forward(
      hidden_states,
      attention_mask,
      encoder_hidden_states,
      encoder_attention_mask,
      past_key_value,
      output_attentions,
      use_cache
    ) → tuple
```

**12.1.3 Complete Forward Pass**

```
def forward(
  self,
  hidden_states: torch.Tensor,                    # [B, seq_len, 512]
  attention_mask: Optional[torch.Tensor] = None,  # Causal mask
  encoder_hidden_states: Optional[torch.Tensor] = None,  # Vision [B, 196, 768]
  encoder_attention_mask: Optional[torch.Tensor] = None,
  past_key_value: Optional[Tuple[torch.Tensor]] = None,
  output_attentions: bool = False,
  use_cache: bool = False
) -> Tuple[torch.Tensor, ...]:
  """
  Single decoder layer forward pass.
  """

  residual = hidden_states

  # ===== Self-Attention Block =====

  # Pre-norm
  hidden_states = self.input_layernorm(hidden_states)

  # Self-attention with causal mask
  self_attn_outputs = self.self_attn(
    hidden_states=hidden_states,
    attention_mask=attention_mask,
    past_key_value=past_key_value[:2] if past_key_value is not None else None,
    output_attentions=output_attentions,
    use_cache=use_cache
  )

  attn_output = self_attn_outputs[0]
  self_attn_weights = self_attn_outputs[1] if output_attentions else None
  present_key_value = self_attn_outputs[-1] if use_cache else None

  # Residual connection
  hidden_states = residual + attn_output

  # ===== Cross-Attention Block (if encoder present) =====

  cross_attn_present_key_value = None
  cross_attn_weights = None

  if encoder_hidden_states is not None:
    residual = hidden_states

    # Pre-norm
    hidden_states = self.post_attention_layernorm(hidden_states)

    # Cross-attention to vision tokens
    cross_attn_outputs = self.cross_attn(
      hidden_states=hidden_states,
      encoder_hidden_states=encoder_hidden_states,
      encoder_attention_mask=encoder_attention_mask,
      past_key_value=past_key_value[2:] if past_key_value is not None else None,
      output_attentions=output_attentions,
      use_cache=use_cache
    )

    attn_output = cross_attn_outputs[0]
    cross_attn_weights = cross_attn_outputs[1] if output_attentions else None
    cross_attn_present_key_value = cross_attn_outputs[-1] if use_cache else None

    # Residual connection
    hidden_states = residual + attn_output

  # ===== Feed-Forward Block =====

  residual = hidden_states

  # Pre-norm
  hidden_states = self.post_cross_attention_layernorm(hidden_states)

  # FFN
  hidden_states = self.mlp(hidden_states)

  # Residual connection
  hidden_states = residual + hidden_states

  # ===== Prepare outputs =====

  outputs = (hidden_states,)

  if output_attentions:
    outputs += (self_attn_weights, cross_attn_weights)

  if use_cache:
    # Combine self-attn and cross-attn cache
    if cross_attn_present_key_value is not None:
      present_key_value = present_key_value + cross_attn_present_key_value
    outputs += (present_key_value,)

  return outputs

Key Architectural Choices:
  - Three residual connections (self-attn, cross-attn, FFN)
  - Three normalizations (before each sub-block)
  - Cross-attention is optional (only if vision tokens provided)
  - Cache stored separately for self and cross attention
```

#### **12.2 Self-Attention with GQA**

**12.2.1 Grouped Query Attention (GQA)**

```
Standard Multi-Head Attention:
  - num_heads = 8
  - Each head has its own Q, K, V
  - Total: 8 Q heads, 8 K heads, 8 V heads

Grouped Query Attention:
  - num_query_heads = 8
  - num_key_value_heads = 2
  - Ratio: 4:1
  - K and V are shared across groups
  - Total: 8 Q heads, 2 K heads, 2 V heads

Grouping:
  Q heads [0,1,2,3] use K,V head 0
  Q heads [4,5,6,7] use K,V head 1

Benefits:
  - Fewer parameters (less K, V projections)
  - Faster inference (smaller KV-cache)
  - Minimal accuracy loss

Parameter Reduction:
  Standard MHA: 3 × hidden_size² = 3 × 512² = 786K
  GQA (4:1): Q:512², K,V:2×(512×128) = 262K + 131K = 393K
  Savings: 50%
```

**12.2.2 GQA Implementation**

```
Class: GemmaSelfAttention(nn.Module)

def __init__(self, config):
  super().__init__()

  self.hidden_size = config.hidden_size  # 512
  self.num_heads = config.num_attention_heads  # 8
  self.num_key_value_heads = config.num_key_value_heads  # 2
  self.head_dim = config.hidden_size // config.num_attention_heads  # 64

  # Number of query heads per KV head
  self.num_key_value_groups = self.num_heads // self.num_key_value_heads  # 4

  # Query projection (full)
  self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)

  # Key, Value projections (reduced)
  self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
  self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)

  # Output projection
  self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)

  # RoPE
  self.rotary_emb = RotaryEmbedding(self.head_dim, config.rope_theta)

def forward(self, hidden_states, ...):
  batch_size, seq_len, _ = hidden_states.shape

  # Project to Q, K, V
  query = self.q_proj(hidden_states)  # [B, seq_len, 8×64=512]
  key = self.k_proj(hidden_states)    # [B, seq_len, 2×64=128]
  value = self.v_proj(hidden_states)  # [B, seq_len, 2×64=128]

  # Reshape for multi-head
  query = query.view(batch_size, seq_len, self.num_heads, self.head_dim)
  query = query.transpose(1, 2)  # [B, 8, seq_len, 64]

  key = key.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)
  key = key.transpose(1, 2)  # [B, 2, seq_len, 64]

  value = value.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)
  value = value.transpose(1, 2)  # [B, 2, seq_len, 64]

  # Apply RoPE to Q and K
  cos, sin = self.rotary_emb(value, seq_len=seq_len)
  query = apply_rotary_pos_emb(query, cos, sin)
  key = apply_rotary_pos_emb(key, cos, sin)

  # Handle KV-cache
  if past_key_value is not None:
    key = torch.cat([past_key_value[0], key], dim=2)
    value = torch.cat([past_key_value[1], value], dim=2)

  if use_cache:
    present_key_value = (key, value)

  # Repeat KV heads to match Q heads
  # [B, 2, seq_len, 64] → [B, 8, seq_len, 64]
  key = repeat_kv(key, self.num_key_value_groups)
  value = repeat_kv(value, self.num_key_value_groups)

  # Attention computation
  attn_weights = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.head_dim)

  # Apply causal mask
  if attention_mask is not None:
    attn_weights = attn_weights + attention_mask

  attn_weights = F.softmax(attn_weights, dim=-1)
  attn_output = torch.matmul(attn_weights, value)

  # Reshape and project
  attn_output = attn_output.transpose(1, 2).contiguous()
  attn_output = attn_output.view(batch_size, seq_len, self.hidden_size)
  attn_output = self.o_proj(attn_output)

  return (attn_output, attn_weights, present_key_value)

Helper Function:
def repeat_kv(hidden_states, n_rep):
  """
  Repeat KV heads to match number of Q heads.

  Args:
    hidden_states: [B, num_kv_heads, seq_len, head_dim]
    n_rep: Repetition factor

  Returns:
    repeated: [B, num_kv_heads * n_rep, seq_len, head_dim]
  """
  batch, num_kv_heads, slen, head_dim = hidden_states.shape

  if n_rep == 1:
    return hidden_states

  # Repeat each KV head n_rep times
  hidden_states = hidden_states[:, :, None, :, :].expand(
    batch, num_kv_heads, n_rep, slen, head_dim
  )

  # Merge into single dimension
  return hidden_states.reshape(batch, num_kv_heads * n_rep, slen, head_dim)
```

#### **12.3 Cross-Attention Implementation**

**12.3.1 Cross-Attention Mechanics**

```
Cross-Attention differs from Self-Attention:

Self-Attention:
  Q, K, V all come from same source (text tokens)

Cross-Attention:
  Q comes from text (decoder)
  K, V come from vision (encoder)

Purpose:
  - Allow text tokens to "look at" image patches
  - Each word can attend to relevant image regions
  - This is the KEY to VLM functionality

Example:
  Command: "Is there a pedestrian?"
  Word "pedestrian" should attend to person regions in image

  Attention weights [text_len, image_patches]:
  "is"         →  distributed attention (global)
  "there"      →  distributed attention
  "a"          →  distributed attention
  "pedestrian" →  HIGH attention on person patches
  "?"          →  distributed attention

This is what makes the model "ground" language in vision!
```

**12.3.2 Cross-Attention Implementation**

```
Class: GemmaCrossAttention(nn.Module)

def __init__(self, config):
  super().__init__()

  self.hidden_size = config.hidden_size  # 512 (text)
  self.num_heads = config.num_attention_heads  # 8
  self.num_key_value_heads = config.num_key_value_heads  # 2
  self.head_dim = self.hidden_size // self.num_heads  # 64
  self.num_key_value_groups = self.num_heads // self.num_key_value_heads

  # Query from text (decoder hidden_size)
  self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)

  # Key, Value from vision (encoder hidden_size might differ!)
  encoder_hidden_size = config.encoder_hidden_size  # 768 (vision)
  self.k_proj = nn.Linear(encoder_hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
  self.v_proj = nn.Linear(encoder_hidden_size, self.num_key_value_heads * self.head_dim, bias=False)

  # Output projection
  self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)

def forward(
  self,
  hidden_states,               # Text: [B, text_len, 512]
  encoder_hidden_states,       # Vision: [B, 196, 768]
  encoder_attention_mask=None,
  past_key_value=None,
  output_attentions=False,
  use_cache=False
):
  batch_size, text_len, _ = hidden_states.shape
  _, image_len, _ = encoder_hidden_states.shape

  # Query from text
  query = self.q_proj(hidden_states)  # [B, text_len, 512]
  query = query.view(batch_size, text_len, self.num_heads, self.head_dim)
  query = query.transpose(1, 2)  # [B, 8, text_len, 64]

  # Key, Value from vision (only compute once, then cache)
  if past_key_value is None:
    key = self.k_proj(encoder_hidden_states)  # [B, 196, 128]
    value = self.v_proj(encoder_hidden_states)  # [B, 196, 128]

    key = key.view(batch_size, image_len, self.num_key_value_heads, self.head_dim)
    key = key.transpose(1, 2)  # [B, 2, 196, 64]

    value = value.view(batch_size, image_len, self.num_key_value_heads, self.head_dim)
    value = value.transpose(1, 2)  # [B, 2, 196, 64]
  else:
    # Reuse cached K, V (vision doesn't change during generation)
    key, value = past_key_value

  if use_cache:
    present_key_value = (key, value)

  # Repeat KV to match Q heads
  key = repeat_kv(key, self.num_key_value_groups)  # [B, 8, 196, 64]
  value = repeat_kv(value, self.num_key_value_groups)  # [B, 8, 196, 64]

  # Cross-attention scores
  # [B, 8, text_len, 64] @ [B, 8, 64, 196] = [B, 8, text_len, 196]
  attn_weights = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.head_dim)

  # Apply encoder mask (if any)
  if encoder_attention_mask is not None:
    attn_weights = attn_weights + encoder_attention_mask

  # Softmax over image patches (last dimension)
  attn_weights = F.softmax(attn_weights, dim=-1)

  # Apply attention to values
  # [B, 8, text_len, 196] @ [B, 8, 196, 64] = [B, 8, text_len, 64]
  attn_output = torch.matmul(attn_weights, value)

  # Reshape and project
  attn_output = attn_output.transpose(1, 2).contiguous()
  attn_output = attn_output.view(batch_size, text_len, self.hidden_size)
  attn_output = self.o_proj(attn_output)

  outputs = (attn_output,)
  if output_attentions:
    outputs += (attn_weights,)
  if use_cache:
    outputs += (present_key_value,)

  return outputs

Key Points:
  - Query dimension: text_len
  - Key/Value dimension: image_len (196)
  - Attention matrix: [text_len × 196]
  - Each text token attends to all image patches
  - No causal mask (text can see all vision)
  - Vision K,V cached after first computation (they don't change)
```

**12.3.3 Dimension Mismatch Handling**

```
Problem: Vision encoder output (768 dim) ≠ Decoder hidden size (512 dim)

Solution: Projection layer before cross-attention

In VLM assembly (Task 17):
  vision_output = vision_encoder(images)  # [B, 196, 768]
  projected_vision = projection_layer(vision_output)  # [B, 196, 512]

  decoder_output = decoder(
    input_ids,
    encoder_hidden_states=projected_vision  # Now matches!
  )

Alternative: Cross-attention handles mismatch internally
  - K,V projections take 768-dim input
  - Q projection takes 512-dim input
  - All meet in 64-dim head space

For this project: Use projection layer (cleaner, more standard)
```

#### **12.4 Feed-Forward Network (SwiGLU)**

**12.4.1 SwiGLU Activation**

```
Standard FFN (GELU):
  FFN(x) = W2(GELU(W1(x)))

SwiGLU FFN:
  FFN(x) = (Swish(W1(x)) ⊙ W3(x)) W2

  Where:
    Swish(x) = x × sigmoid(x)
    ⊙ = element-wise multiplication
    W1, W3 = two separate uprojections
    W2 = down projection

Why SwiGLU?
  - Better performance than GELU/ReLU
  - Used in LLaMA, Gemma, modern LLMs
  - Gating mechanism (like LSTM gates)
  - More expressive

Parameters:
  W1: hidden_size → intermediate_size (512 → 2048)
  W3: hidden_size → intermediate_size (512 → 2048)
  W2: intermediate_size → hidden_size (2048 → 512)

  Total: 2 × 512 × 2048 + 2048 × 512 = 3.14M params
```

**12.4.2 SwiGLU Implementation**

```
Class: GemmaFeedForward(nn.Module)

def __init__(self, config):
  super().__init__()

  self.hidden_size = config.hidden_size  # 512
  self.intermediate_size = config.intermediate_size  # 2048

  # Two uprojections (gate and input)
  self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
  self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)

  # Downprojection
  self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)

  # Activation
  self.act_fn = nn.SiLU()  # SiLU = Swish

def forward(self, hidden_states):
  """
  Args:
    hidden_states: [B, seq_len, 512]

  Returns:
    output: [B, seq_len, 512]
  """

  # Gate branch: apply activation
  gate = self.act_fn(self.gate_proj(hidden_states))  # [B, seq_len, 2048]

  # Input branch: no activation
  up = self.up_proj(hidden_states)  # [B, seq_len, 2048]

  # Element-wise multiplication (gating)
  intermediate = gate * up  # [B, seq_len, 2048]

  # Project back down
  output = self.down_proj(intermediate)  # [B, seq_len, 512]

  return output

Visualization of gating:
  If gate[i] ≈ 1 → information flows through
  If gate[i] ≈ 0 → information blocked

  This allows network to dynamically route information
```

#### **12.5 RMS Normalization**

**12.5.1 RMS Norm vs Layer Norm**

```
Layer Normalization:
  mean = E[x]
  var = E[(x - mean)²]
  output = (x - mean) / sqrt(var + eps) * gamma + beta

  Parameters: gamma (scale), beta (shift)

RMS Normalization:
  rms = sqrt(E[x²])
  output = x / (rms + eps) * gamma

  Parameters: only gamma (scale)
  No mean subtraction, no beta

Why RMS Norm?
  - Simpler (no mean subtraction)
  - Faster (fewer operations)
  - Works as well or better empirically
  - Used in LLaMA, Gemma, modern LLMs

Computation Savings:
  LayerNorm: 2 passes (mean, then variance)
  RMSNorm: 1 pass (RMS directly)
```

**12.5.2 RMS Norm Implementation**

```
Class: RMSNorm(nn.Module)

def __init__(self, hidden_size, eps=1e-6):
  super().__init__()
  self.weight = nn.Parameter(torch.ones(hidden_size))
  self.variance_epsilon = eps

def forward(self, hidden_states):
  """
  Args:
    hidden_states: [B, seq_len, hidden_size]

  Returns:
    normed: [B, seq_len, hidden_size]
  """
  input_dtype = hidden_states.dtype

  # Cast to float32 for numerical stability
  hidden_states = hidden_states.to(torch.float32)

  # Compute RMS
  # E[x²] along last dimension
  variance = hidden_states.pow(2).mean(dim=-1, keepdim=True)

  # RMS = sqrt(E[x²] + eps)
  hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)

  # Apply learned scale (gamma)
  hidden_states = self.weight * hidden_states

  # Cast back to original dtype
  return hidden_states.to(input_dtype)

Example:
  rms_norm = RMSNorm(hidden_size=512)

  input = torch.randn(2, 10, 512)
  output = rms_norm(input)

  # Check normalization (approximately)
  print(output.pow(2).mean(dim=-1))  # Should be close to 1.0
```

---

### **Task 13: rope_embeddings.py - Rotary Position Embeddings**

#### **13.1 RoPE Overview**

**13.1.1 Why RoPE?**

```
Problem with absolute position embeddings:
  - Added at input only
  - Position info dilutes through layers
  - Poor extrapolation to longer sequences

RoPE (Rotary Position Embedding):
  - Apply position info in attention computation itself
  - Rotate Q and K by position-dependent angles
  - Relative position naturally emerges from rotation
  - Better extrapolation to longer sequences

Key Idea:
  Encode position by rotating vectors in 2D planes

Mathematical Intuition:
  Position i → rotation by angle θ × i
  Position j → rotation by angle θ × j

  When computing Q_i · K_j:
    Dot product captures relative distance (i - j)

  This is like complex number multiplication!
```

**13.1.2 RoPE Mathematics**

```
For each head dimension pair (2D rotation):

Define rotation matrix R(θ, m):
  R(θ, m) = [cos(mθ)  -sin(mθ)]
            [sin(mθ)   cos(mθ)]

Where:
  m = position index (0, 1, 2, ...)
  θ = base frequency (depends on dimension)

For dimension d, frequency:
  θ_d = base^(-2d/D)
  base = 10000 (configurable)
  D = head_dim (64)

Apply to Query and Key:
  Q_m = R(θ, m) @ Q  (rotate query at position m)
  K_n = R(θ, n) @ K  (rotate key at position n)

When computing attention:
  Q_m · K_n = (R(θ,m) @ Q) · (R(θ,n) @ K)
            = Q^T @ R(θ,m)^T @ R(θ,n) @ K
            = Q^T @ R(θ, n-m) @ K

  The dot product depends only on relative distance (n-m)!

Example:
  Token at position 5 attending to position 3:
    Relative position = 3 - 5 = -2

  Token at position 10 attending to position 8:
    Relative position = 8 - 10 = -2

  Same relative attention pattern!
```

**13.1.3 RoPE Implementation**

```
Class: RotaryEmbedding(nn.Module)

def __init__(self, dim, base=10000.0, device=None):
  """
  Args:
    dim: Head dimension (64)
    base: Base frequency (10000)
  """
  super().__init__()

  self.dim = dim
  self.base = base

  # Precompute frequency for each dimension pair
  # dim = 64 → 32 pairs
  inv_freq = 1.0 / (self.base ** (torch.arange(0, dim, 2).float() / dim))
  # Shape: [32]

  # Register as buffer (not trained)
  self.register_buffer("inv_freq", inv_freq)

def forward(self, x, seq_len):
  """
  Args:
    x: dummy tensor for device/dtype
    seq_len: Sequence length

  Returns:
    cos, sin: Precomputed cos and sin for all positions
  """
  # Create position indices [0, 1, 2, ..., seq_len-1]
  t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)

  # Compute angles: position × frequency
  # [seq_len, 1] × [1, dim//2] = [seq_len, dim//2]
  freqs = torch.outer(t, self.inv_freq)

  # Concatenate to get full dimensions
  # [seq_len, dim//2] → [seq_len, dim]
  emb = torch.cat((freqs, freqs), dim=-1)

  # Compute cos and sin
  cos = emb.cos()  # [seq_len, dim]
  sin = emb.sin()  # [seq_len, dim]

  return cos, sin

Apply RoPE to Query/Key:

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None):
  """
  Apply rotary position embedding to Q and K.

  Args:
    q: Query [B, num_heads, seq_len, head_dim]
    k: Key [B, num_heads, seq_len, head_dim]
    cos: Precomputed cos [seq_len, head_dim]
    sin: Precomputed sin [seq_len, head_dim]

  Returns:
    q_embed, k_embed: Rotated Q and K
  """
  # Add dimensions for broadcasting
  cos = cos.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, head_dim]
  sin = sin.unsqueeze(0).unsqueeze(0)

  # Rotate Q
  q_embed = (q * cos) + (rotate_half(q) * sin)

  # Rotate K
  k_embed = (k * cos) + (rotate_half(k) * sin)

  return q_embed, k_embed

def rotate_half(x):
  """
  Rotate half the hidden dims of the input.

  This implements the 2D rotation by swapping pairs and negating.

  Args:
    x: [B, num_heads, seq_len, head_dim]

  Returns:
    x_rotated: Same shape
  """
  x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]
  return torch.cat((-x2, x1), dim=-1)

Example:
  For head_dim = 4:
    x = [a, b, c, d]
    rotate_half(x) = [-c, -d, a, b]

  This implements swapping pairs and negating first of each pair
```

**13.1.4 Numerical Example**

```
Let's trace through with small numbers:

dim = 4 (instead of 64)
seq_len = 3
base = 10000

Step 1: Compute inverse frequencies
  inv_freq = 1 / (10000 ^ [0/4, 2/4])
           = 1 / [1, 100]
           = [1.0, 0.01]

Step 2: Compute angles for each position
  positions = [0, 1, 2]

  Position 0: freqs = [0×1.0, 0×0.01] = [0.0, 0.0]
  Position 1: freqs = [1×1.0, 1×0.01] = [1.0, 0.01]
  Position 2: freqs = [2×1.0, 2×0.01] = [2.0, 0.02]

  Concatenate: [0.0, 0.0, 0.0, 0.0]
               [1.0, 0.01, 1.0, 0.01]
               [2.0, 0.02, 2.0, 0.02]

Step 3: Compute cos and sin
  Position 0: cos=[1.0, 1.0, 1.0, 1.0], sin=[0.0, 0.0, 0.0, 0.0]
  Position 1: cos=[0.54, 1.0, 0.54, 1.0], sin=[0.84, 0.01, 0.84, 0.01]
  Position 2: cos=[-0.42, 1.0, -0.42, 1.0], sin=[0.91, 0.02, 0.91, 0.02]

Step 4: Apply to query at position 1
  q = [3.5, 2.1, -1.2, 0.8]
  cos = [0.54, 1.0, 0.54, 1.0]
  sin = [0.84, 0.01, 0.84, 0.01]

  rotate_half(q) = [1.2, -0.8, 3.5, 2.1]

  q_embed = q * cos + rotate_half(q) * sin
          = [3.5×0.54, 2.1×1.0, -1.2×0.54, 0.8×1.0]
          + [1.2×0.84, -0.8×0.01, 3.5×0.84, 2.1×0.01]
          = [1.89, 2.1, -0.65, 0.8] + [1.01, -0.008, 2.94, 0.021]
          = [2.90, 2.09, 2.29, 0.82]

Position information is now encoded in the rotated vectors!
```

#### **13.2 RoPE with KV-Cache**

**13.2.1 Position Offset Handling**

```
Problem during generation with cache:

Step 1 (prefill):
  input = [token_0, token_1, token_2]
  positions = [0, 1, 2]
  Apply RoPE with these positions ✓

Step 2 (generate token_3):
  input = [token_3]  (only new token)
  positions = [3]  ← NOT [0]!

  Must apply RoPE with position_id=3
  Then concatenate with cached K,V from positions [0,1,2]

Solution: Track position offset

def forward(self, x, seq_len, past_key_values_length=0):
  # Total sequence length including cache
  total_len = seq_len + past_key_values_length

  # Positions for current tokens
  positions = torch.arange(
    past_key_values_length,
    total_len,
    device=x.device
  )

  # Compute cos, sin for these positions
  freqs = torch.outer(positions, self.inv_freq)
  emb = torch.cat((freqs, freqs), dim=-1)
  cos = emb.cos()
  sin = emb.sin()

  return cos, sin

Usage in attention:
  # Get position offset from cache
  past_len = 0 if past_key_value is None else past_key_value[0].shape[2]

  # Compute RoPE
  cos, sin = self.rotary_emb(value, seq_len=seq_len, past_key_values_length=past_len)

  # Apply to current Q, K only
  query = apply_rotary_pos_emb(query, cos, sin)
  key = apply_rotary_pos_emb(key, cos, sin)

  # Then concatenate with cached K, V
  if past_key_value is not None:
    key = torch.cat([past_key_value[0], key], dim=2)
    value = torch.cat([past_key_value[1], value], dim=2)
```

#### **13.3 Testing RoPE**

**13.3.1 RoPE Tests**

```
1. test_rope_shape():
   rope = RotaryEmbedding(dim=64)
   x = torch.randn(1, 1, 10, 64)
   cos, sin = rope(x, seq_len=10)

   assert cos.shape == (10, 64)
   assert sin.shape == (10, 64)

2. test_rope_rotation():
   # Position 0 should not rotate
   cos0, sin0 = rope(x, seq_len=1)
   assert torch.allclose(cos0[0], torch.ones(64))
   assert torch.allclose(sin0[0], torch.zeros(64), atol=1e-6)

3. test_apply_rope():
   q = torch.randn(1, 8, 10, 64)
   k = torch.randn(1, 8, 10, 64)
   cos, sin = rope(q, seq_len=10)

   q_rot, k_rot = apply_rotary_pos_emb(q, k, cos, sin)

   assert q_rot.shape == q.shape
   assert k_rot.shape == k.shape

4. test_rope_relative_position():
   # Same relative distance should have similar attention
   q = torch.randn(1, 1, 5, 64)
   k = torch.randn(1, 1, 5, 64)
   cos, sin = rope(q, seq_len=5)

   q_rot, k_rot = apply_rotary_pos_emb(q, k, cos, sin)

   # Attention from position 2 to position 0 (distance -2)
   attn_2_0 = (q_rot[0, 0, 2] * k_rot[0, 0, 0]).sum()

   # Attention from position 4 to position 2 (also distance -2)
   attn_4_2 = (q_rot[0, 0, 4] * k_rot[0, 0, 2]).sum()

   # Should be similar (relative position encoding)
   # Not exactly equal due to different absolute positions
   # But correlation should be high
   print(f"Attention 2→0: {attn_2_0:.4f}")
   print(f"Attention 4→2: {attn_4_2:.4f}")
```

---
