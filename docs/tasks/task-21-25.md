# Traffic Scene VLM - ULTRA-DETAILED Tasks 21-25 (Continued)

---

## **PHASE 6: TRAINING PIPELINE (Continued)**

---

### **Task 21: evaluation.py - Model Evaluation & Metrics**

#### **21.1 Validation Loop**

**21.1.1 Validation Function**

```
def validate(model, dataloader, device="cuda"):
  """
  Evaluate model on validation set.

  Args:
    model: TrafficVLM instance
    dataloader: Validation data loader
    device: Device for evaluation

  Returns:
    metrics: Dict of evaluation metrics
  """
  model.eval()

  total_loss = 0.0
  all_predictions = []
  all_labels = []
  all_logits = []

  with torch.no_grad():
    for batch in tqdm(dataloader, desc="Validating"):
      # Move batch to device
      images = batch["images"].to(device)
      input_ids = batch["input_ids"].to(device)
      attention_mask = batch["attention_mask"].to(device)
      labels = batch["labels"].to(device)

      # Forward pass
      outputs = model(
        pixel_values=images,
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=labels
      )

      loss = outputs["loss"]
      logits = outputs["logits"]

      # Predictions
      predictions = torch.argmax(logits, dim=-1)

      # Accumulate
      total_loss += loss.item() * images.size(0)
      all_predictions.extend(predictions.cpu().numpy())
      all_labels.extend(labels.cpu().numpy())
      all_logits.extend(logits.cpu().numpy())

  # Convert to numpy arrays
  all_predictions = np.array(all_predictions)
  all_labels = np.array(all_labels)
  all_logits = np.array(all_logits)

  # Compute metrics
  avg_loss = total_loss / len(dataloader.dataset)
  accuracy = (all_predictions == all_labels).mean()

  # Detailed metrics
  metrics = compute_classification_metrics(
    all_labels,
    all_predictions,
    all_logits
  )

  metrics["loss"] = avg_loss

  return metrics
```

#### **21.2 Classification Metrics**

**21.2.1 Comprehensive Metrics Function**

```
def compute_classification_metrics(labels, predictions, logits=None):
  """
  Compute comprehensive classification metrics.

  Args:
    labels: Ground truth labels [N]
    predictions: Predicted labels [N]
    logits: Raw logits [N, num_classes] (optional)

  Returns:
    metrics: Dict of metrics
  """
  from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report,
    roc_auc_score,
    balanced_accuracy_score
  )

  metrics = {}

  # ===== Basic Metrics =====

  # Accuracy
  metrics["accuracy"] = accuracy_score(labels, predictions)

  # Balanced Accuracy (better for imbalanced data)
  metrics["balanced_accuracy"] = balanced_accuracy_score(labels, predictions)

  # Precision, Recall, F1 (for each class and macro average)
  metrics["precision_macro"] = precision_score(labels, predictions, average="macro", zero_division=0)
  metrics["recall_macro"] = recall_score(labels, predictions, average="macro", zero_division=0)
  metrics["f1_macro"] = f1_score(labels, predictions, average="macro", zero_division=0)

  # Per-class metrics
  num_classes = len(np.unique(labels))
  for class_id in range(num_classes):
    class_name = ["NO", "YES"][class_id]  # For binary
    metrics[f"precision_{class_name}"] = precision_score(
      labels, predictions, labels=[class_id], average=None, zero_division=0
    )[0] if class_id in predictions else 0.0

    metrics[f"recall_{class_name}"] = recall_score(
      labels, predictions, labels=[class_id], average=None, zero_division=0
    )[0] if class_id in labels else 0.0

    metrics[f"f1_{class_name}"] = f1_score(
      labels, predictions, labels=[class_id], average=None, zero_division=0
    )[0] if class_id in labels and class_id in predictions else 0.0

  # ===== Confusion Matrix =====
  cm = confusion_matrix(labels, predictions)
  metrics["confusion_matrix"] = cm

  # True Negatives, False Positives, False Negatives, True Positives
  if num_classes == 2:
    tn, fp, fn, tp = cm.ravel()
    metrics["true_negatives"] = int(tn)
    metrics["false_positives"] = int(fp)
    metrics["false_negatives"] = int(fn)
    metrics["true_positives"] = int(tp)

    # Specificity (True Negative Rate)
    metrics["specificity"] = tn / (tn + fp) if (tn + fp) > 0 else 0.0

    # Sensitivity (same as recall/TPR)
    metrics["sensitivity"] = tp / (tp + fn) if (tp + fn) > 0 else 0.0

  # ===== AUC-ROC =====
  if logits is not None:
    try:
      # Convert logits to probabilities
      probs = softmax(logits, axis=1)

      if num_classes == 2:
        # Binary classification: use positive class probability
        metrics["auc_roc"] = roc_auc_score(labels, probs[:, 1])
      else:
        # Multi-class: macro average
        metrics["auc_roc"] = roc_auc_score(
          labels, probs, multi_class="ovr", average="macro"
        )
    except Exception as e:
      print(f"Warning: Could not compute AUC-ROC: {e}")
      metrics["auc_roc"] = 0.0

  # ===== Classification Report (detailed) =====
  report = classification_report(
    labels,
    predictions,
    target_names=["NO", "YES"],
    output_dict=True,
    zero_division=0
  )
  metrics["classification_report"] = report

  return metrics

def softmax(x, axis=-1):
  """Numpy softmax implementation."""
  exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
  return exp_x / np.sum(exp_x, axis=axis, keepdims=True)
```

**21.2.2 Metric Summary Printing**

```
def print_metrics(metrics, title="Metrics"):
  """Pretty print metrics."""
  print(f"\n{'='*80}")
  print(f"{title}")
  print(f"{'='*80}")

  # Overall metrics
  print(f"\nOverall Performance:")
  print(f"  Accuracy:          {metrics['accuracy']:.4f}")
  print(f"  Balanced Accuracy: {metrics['balanced_accuracy']:.4f}")
  print(f"  F1 Score (macro):  {metrics['f1_macro']:.4f}")
  print(f"  Precision (macro): {metrics['precision_macro']:.4f}")
  print(f"  Recall (macro):    {metrics['recall_macro']:.4f}")

  if "auc_roc" in metrics:
    print(f"  AUC-ROC:           {metrics['auc_roc']:.4f}")

  # Per-class metrics
  print(f"\nPer-Class Performance:")
  for class_name in ["NO", "YES"]:
    print(f"  {class_name}:")
    print(f"    Precision: {metrics[f'precision_{class_name}']:.4f}")
    print(f"    Recall:    {metrics[f'recall_{class_name}']:.4f}")
    print(f"    F1 Score:  {metrics[f'f1_{class_name}']:.4f}")

  # Confusion matrix
  if "confusion_matrix" in metrics:
    cm = metrics["confusion_matrix"]
    print(f"\nConfusion Matrix:")
    print(f"                Predicted NO    Predicted YES")
    print(f"  Actual NO     {cm[0,0]:>12}    {cm[0,1]:>13}")
    print(f"  Actual YES    {cm[1,0]:>12}    {cm[1,1]:>13}")

  # Binary-specific metrics
  if "true_positives" in metrics:
    print(f"\nBinary Classification Metrics:")
    print(f"  True Positives:  {metrics['true_positives']}")
    print(f"  True Negatives:  {metrics['true_negatives']}")
    print(f"  False Positives: {metrics['false_positives']}")
    print(f"  False Negatives: {metrics['false_negatives']}")
    print(f"  Sensitivity:     {metrics['sensitivity']:.4f}")
    print(f"  Specificity:     {metrics['specificity']:.4f}")

  print(f"{'='*80}\n")

Example output:
  ================================================================================
  Validation Metrics
  ================================================================================

  Overall Performance:
    Accuracy:          0.8750
    Balanced Accuracy: 0.8700
    F1 Score (macro):  0.8720
    Precision (macro): 0.8800
    Recall (macro):    0.8700
    AUC-ROC:           0.9200

  Per-Class Performance:
    NO:
      Precision: 0.8500
      Recall:    0.8900
      F1 Score:  0.8690
    YES:
      Precision: 0.9100
      Recall:    0.8500
      F1 Score:  0.8750

  Confusion Matrix:
                  Predicted NO    Predicted YES
    Actual NO              178               22
    Actual YES              45              255

  Binary Classification Metrics:
    True Positives:  255
    True Negatives:  178
    False Positives: 22
    False Negatives: 45
    Sensitivity:     0.8500
    Specificity:     0.8900
  ================================================================================
```

#### **21.3 Per-Command-Type Analysis**

**21.3.1 Breakdown by Command Type**

```
def evaluate_by_command_type(model, dataloader, device="cuda"):
  """
  Evaluate model performance by command type.

  Returns metrics for each command category:
    - Detection ("Is there a...?")
    - Safety ("Can the car...?")
    - Counting ("How many...?")
    - Spatial ("What is on the left...?")
  """
  model.eval()

  # Store predictions by type
  results_by_type = defaultdict(lambda: {
    "labels": [],
    "predictions": [],
    "logits": []
  })

  with torch.no_grad():
    for batch in tqdm(dataloader, desc="Evaluating by type"):
      images = batch["images"].to(device)
      input_ids = batch["input_ids"].to(device)
      attention_mask = batch["attention_mask"].to(device)
      labels = batch["labels"].to(device)
      command_types = batch["command_types"]  # List of strings

      # Forward pass
      outputs = model(images, input_ids, attention_mask)
      logits = outputs["logits"]
      predictions = torch.argmax(logits, dim=-1)

      # Group by command type
      for i in range(len(labels)):
        cmd_type = command_types[i]
        results_by_type[cmd_type]["labels"].append(labels[i].item())
        results_by_type[cmd_type]["predictions"].append(predictions[i].item())
        results_by_type[cmd_type]["logits"].append(logits[i].cpu().numpy())

  # Compute metrics per type
  metrics_by_type = {}

  for cmd_type, results in results_by_type.items():
    labels = np.array(results["labels"])
    predictions = np.array(results["predictions"])
    logits = np.array(results["logits"])

    metrics = compute_classification_metrics(labels, predictions, logits)
    metrics_by_type[cmd_type] = metrics

  return metrics_by_type

def print_command_type_analysis(metrics_by_type):
  """Print breakdown by command type."""
  print(f"\n{'='*80}")
  print("Performance by Command Type")
  print(f"{'='*80}\n")

  for cmd_type, metrics in sorted(metrics_by_type.items()):
    print(f"{cmd_type.upper()}:")
    print(f"  Accuracy:  {metrics['accuracy']:.4f}")
    print(f"  F1 Score:  {metrics['f1_macro']:.4f}")
    print(f"  Samples:   {len(metrics['confusion_matrix'].ravel())}")
    print()

  print(f"{'='*80}\n")
```

#### **21.4 Error Analysis**

**21.4.1 Analyzing Misclassifications**

```
def analyze_errors(model, dataloader, tokenizer, device="cuda", num_examples=20):
  """
  Analyze model errors in detail.

  Returns:
    error_cases: List of misclassified examples with details
  """
  model.eval()

  error_cases = []

  with torch.no_grad():
    for batch in dataloader:
      images = batch["images"].to(device)
      input_ids = batch["input_ids"].to(device)
      attention_mask = batch["attention_mask"].to(device)
      labels = batch["labels"].to(device)
      image_ids = batch["image_ids"]
      command_texts = batch["command_texts"]

      # Forward pass
      outputs = model(images, input_ids, attention_mask, output_attentions=True)
      logits = outputs["logits"]
      predictions = torch.argmax(logits, dim=-1)
      probs = torch.softmax(logits, dim=-1)

      # Find errors
      errors = (predictions != labels).cpu().numpy()

      for i in np.where(errors)[0]:
        error_case = {
          "image_id": image_ids[i],
          "command": command_texts[i],
          "true_label": ["NO", "YES"][labels[i].item()],
          "predicted_label": ["NO", "YES"][predictions[i].item()],
          "confidence": probs[i, predictions[i]].item(),
          "true_prob": probs[i, labels[i]].item(),
          "logits": logits[i].cpu().numpy()
        }

        error_cases.append(error_case)

        if len(error_cases) >= num_examples:
          break

      if len(error_cases) >= num_examples:
        break

  return error_cases

def print_error_analysis(error_cases):
  """Print detailed error analysis."""
  print(f"\n{'='*80}")
  print("Error Analysis - Sample Misclassifications")
  print(f"{'='*80}\n")

  for i, case in enumerate(error_cases[:10], 1):
    print(f"Example {i}:")
    print(f"  Image ID:     {case['image_id']}")
    print(f"  Command:      {case['command']}")
    print(f"  True Label:   {case['true_label']}")
    print(f"  Predicted:    {case['predicted_label']}")
    print(f"  Confidence:   {case['confidence']:.4f}")
    print(f"  True Prob:    {case['true_prob']:.4f}")
    print()

  # Error patterns
  print("Error Pattern Analysis:")

  # High confidence errors
  high_conf_errors = [c for c in error_cases if c['confidence'] > 0.9]
  print(f"  High confidence errors (>0.9): {len(high_conf_errors)}/{len(error_cases)}")

  # Low confidence errors
  low_conf_errors = [c for c in error_cases if c['confidence'] < 0.6]
  print(f"  Low confidence errors (<0.6): {len(low_conf_errors)}/{len(error_cases)}")

  # False positives vs false negatives
  false_positives = [c for c in error_cases if c['predicted_label'] == 'YES']
  false_negatives = [c for c in error_cases if c['predicted_label'] == 'NO']
  print(f"  False Positives: {len(false_positives)}")
  print(f"  False Negatives: {len(false_negatives)}")

  print(f"\n{'='*80}\n")
```

**21.4.2 Confidence Calibration Analysis**

```
def analyze_calibration(labels, predictions, probs):
  """
  Analyze model calibration (confidence vs accuracy).

  Well-calibrated model: 90% confidence â†’ 90% accuracy
  """
  import matplotlib.pyplot as plt

  # Bin by confidence
  bins = np.linspace(0, 1, 11)  # 10 bins
  bin_centers = (bins[:-1] + bins[1:]) / 2

  # Get confidence for predicted class
  confidences = np.max(probs, axis=1)
  correct = (predictions == labels)

  bin_accuracies = []
  bin_confidences = []
  bin_counts = []

  for i in range(len(bins) - 1):
    in_bin = (confidences >= bins[i]) & (confidences < bins[i+1])

    if in_bin.sum() > 0:
      bin_accuracy = correct[in_bin].mean()
      bin_confidence = confidences[in_bin].mean()
      bin_count = in_bin.sum()

      bin_accuracies.append(bin_accuracy)
      bin_confidences.append(bin_confidence)
      bin_counts.append(bin_count)
    else:
      bin_accuracies.append(0)
      bin_confidences.append(bin_centers[i])
      bin_counts.append(0)

  # Expected Calibration Error (ECE)
  ece = 0
  total = len(labels)
  for acc, conf, count in zip(bin_accuracies, bin_confidences, bin_counts):
    ece += (count / total) * abs(acc - conf)

  # Plot calibration curve
  plt.figure(figsize=(8, 8))
  plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')
  plt.plot(bin_confidences, bin_accuracies, 'o-', label=f'Model (ECE={ece:.4f})')
  plt.xlabel('Confidence')
  plt.ylabel('Accuracy')
  plt.title('Calibration Curve')
  plt.legend()
  plt.grid(True, alpha=0.3)
  plt.savefig('outputs/visualizations/calibration_curve.png')
  plt.close()

  print(f"Expected Calibration Error (ECE): {ece:.4f}")
  print("Calibration curve saved to outputs/visualizations/calibration_curve.png")

  return ece

Good ECE: < 0.05
Okay ECE: 0.05 - 0.15
Poor ECE: > 0.15
```

---

### **Task 22: logger.py - Logging & Monitoring**

#### **22.1 Training Logger**

**22.1.1 Logger Class**

```
import logging
from pathlib import Path
from datetime import datetime
import json

class TrainingLogger:
  """Comprehensive training logger."""

  def __init__(self, log_dir="logs", experiment_name=None):
    self.log_dir = Path(log_dir)
    self.log_dir.mkdir(parents=True, exist_ok=True)

    # Experiment name with timestamp
    if experiment_name is None:
      experiment_name = datetime.now().strftime("%Y%m%d_%H%M%S")
    self.experiment_name = experiment_name

    # Setup file logging
    log_file = self.log_dir / f"{experiment_name}.log"
    logging.basicConfig(
      level=logging.INFO,
      format='%(asctime)s - %(levelname)s - %(message)s',
      handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
      ]
    )
    self.logger = logging.getLogger(__name__)

    # Metrics storage
    self.metrics_history = {
      "train": [],
      "val": []
    }

    # Step counter
    self.global_step = 0

    self.logger.info(f"Initialized logger for experiment: {experiment_name}")
    self.logger.info(f"Log file: {log_file}")

  def log_step(self, split, metrics, step=None):
    """
    Log metrics for a training/validation step.

    Args:
      split: "train" or "val"
      metrics: Dict of metrics
      step: Global step number (optional)
    """
    if step is None:
      step = self.global_step
      self.global_step += 1

    # Add to history
    metrics_with_step = {"step": step, **metrics}
    self.metrics_history[split].append(metrics_with_step)

    # Log to console
    metrics_str = ", ".join([f"{k}: {v:.4f}" if isinstance(v, float) else f"{k}: {v}"
                            for k, v in metrics.items()])
    self.logger.info(f"[{split.upper()}] Step {step} - {metrics_str}")

  def log_epoch(self, epoch, train_metrics, val_metrics, lr):
    """Log end-of-epoch summary."""
    self.logger.info(f"\n{'='*80}")
    self.logger.info(f"Epoch {epoch} Summary")
    self.logger.info(f"{'='*80}")
    self.logger.info(f"Train Loss: {train_metrics['loss']:.4f}, "
                    f"Train Acc: {train_metrics['accuracy']:.4f}")
    self.logger.info(f"Val Loss:   {val_metrics['loss']:.4f}, "
                    f"Val Acc:   {val_metrics['accuracy']:.4f}")
    self.logger.info(f"Learning Rate: {lr:.2e}")
    self.logger.info(f"{'='*80}\n")

  def log_hyperparameters(self, config):
    """Log hyperparameters."""
    self.logger.info("\nHyperparameters:")
    for key, value in vars(config).items():
      self.logger.info(f"  {key}: {value}")

    # Save to JSON
    config_file = self.log_dir / f"{self.experiment_name}_config.json"
    with open(config_file, 'w') as f:
      json.dump(vars(config), f, indent=2, default=str)

  def log_model_summary(self, model):
    """Log model architecture summary."""
    self.logger.info("\nModel Summary:")
    self.logger.info(f"Total Parameters: {sum(p.numel() for p in model.parameters()):,}")
    self.logger.info(f"Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

  def save_metrics(self):
    """Save metrics history to JSON."""
    metrics_file = self.log_dir / f"{self.experiment_name}_metrics.json"
    with open(metrics_file, 'w') as f:
      json.dump(self.metrics_history, f, indent=2)
    self.logger.info(f"Metrics saved to {metrics_file}")
```

#### **22.2 TensorBoard Integration**

**22.2.1 TensorBoard Logger**

```
from torch.utils.tensorboard import SummaryWriter

class TensorBoardLogger:
  """TensorBoard logging for training."""

  def __init__(self, log_dir="runs", experiment_name=None):
    if experiment_name is None:
      experiment_name = datetime.now().strftime("%Y%m%d_%H%M%S")

    self.log_dir = Path(log_dir) / experiment_name
    self.writer = SummaryWriter(str(self.log_dir))
    self.experiment_name = experiment_name

    print(f"TensorBoard logging to: {self.log_dir}")
    print(f"Run: tensorboard --logdir={log_dir}")

  def log_scalar(self, tag, value, step):
    """Log scalar value."""
    self.writer.add_scalar(tag, value, step)

  def log_scalars(self, main_tag, tag_scalar_dict, step):
    """Log multiple scalars."""
    self.writer.add_scalars(main_tag, tag_scalar_dict, step)

  def log_metrics(self, split, metrics, step):
    """Log all metrics for a split."""
    for key, value in metrics.items():
      if isinstance(value, (int, float)):
        self.writer.add_scalar(f"{split}/{key}", value, step)

  def log_histogram(self, tag, values, step):
    """Log histogram of values."""
    self.writer.add_histogram(tag, values, step)

  def log_image(self, tag, image, step):
    """Log image."""
    self.writer.add_image(tag, image, step)

  def log_images(self, tag, images, step):
    """Log multiple images as grid."""
    from torchvision.utils import make_grid
    grid = make_grid(images)
    self.writer.add_image(tag, grid, step)

  def log_attention_map(self, attention_weights, step, tag="attention"):
    """
    Visualize cross-attention weights.

    Args:
      attention_weights: [text_len, 196] attention map
      step: Global step
      tag: Tag for logging
    """
    import matplotlib.pyplot as plt

    # Reshape to 2D grid (14x14)
    attention_2d = attention_weights.reshape(attention_weights.shape[0], 14, 14)

    # Create figure
    fig, axes = plt.subplots(1, attention_2d.shape[0], figsize=(3*attention_2d.shape[0], 3))
    if attention_2d.shape[0] == 1:
      axes = [axes]

    for i, ax in enumerate(axes):
      im = ax.imshow(attention_2d[i], cmap='hot')
      ax.set_title(f"Token {i}")
      ax.axis('off')
      plt.colorbar(im, ax=ax, fraction=0.046)

    plt.tight_layout()

    # Log to TensorBoard
    self.writer.add_figure(tag, fig, step)
    plt.close(fig)

  def log_confusion_matrix(self, cm, class_names, step, tag="confusion_matrix"):
    """Log confusion matrix as image."""
    import matplotlib.pyplot as plt
    import seaborn as sns

    fig, ax = plt.subplots(figsize=(8, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names, ax=ax)
    ax.set_xlabel('Predicted')
    ax.set_ylabel('True')
    ax.set_title('Confusion Matrix')

    self.writer.add_figure(tag, fig, step)
    plt.close(fig)

  def log_model_graph(self, model, input_sample):
    """Log model computation graph."""
    self.writer.add_graph(model, input_sample)

  def close(self):
    """Close writer."""
    self.writer.close()

Usage in training loop:
  tb_logger = TensorBoardLogger(log_dir="runs", experiment_name="traffic_vlm_v1")

  for epoch in range(num_epochs):
    for step, batch in enumerate(train_dataloader):
      # Training step
      loss = train_step(model, batch)

      # Log to TensorBoard
      tb_logger.log_scalar("train/loss", loss, global_step)
      tb_logger.log_scalar("train/lr", optimizer.param_groups[0]['lr'], global_step)

      global_step += 1

    # Validation
    val_metrics = validate(model, val_dataloader)
    tb_logger.log_metrics("val", val_metrics, epoch)
    tb_logger.log_confusion_matrix(
      val_metrics['confusion_matrix'],
      ["NO", "YES"],
      epoch
    )

  tb_logger.close()
```

#### **22.3 Weights & Biases Integration**

**22.3.1 WandB Logger**

```
import wandb

class WandBLogger:
  """Weights & Biases logging."""

  def __init__(self, project, experiment_name=None, config=None):
    """
    Initialize WandB logger.

    Args:
      project: WandB project name
      experiment_name: Run name
      config: Training config dict
    """
    wandb.init(
      project=project,
      name=experiment_name,
      config=config
    )

    print(f"WandB initialized: {wandb.run.url}")

  def log(self, metrics, step=None):
    """Log metrics."""
    wandb.log(metrics, step=step)

  def log_image(self, key, image, step=None):
    """Log image."""
    wandb.log({key: wandb.Image(image)}, step=step)

  def log_confusion_matrix(self, cm, class_names):
    """Log confusion matrix."""
    wandb.log({
      "confusion_matrix": wandb.plot.confusion_matrix(
        probs=None,
        y_true=None,  # Will be filled by WandB
        preds=None,
        class_names=class_names
      )
    })

  def log_table(self, name, data, columns):
    """Log data table."""
    table = wandb.Table(data=data, columns=columns)
    wandb.log({name: table})

  def watch_model(self, model, log_freq=100):
    """Watch model gradients and parameters."""
    wandb.watch(model, log="all", log_freq=log_freq)

  def finish(self):
    """Finish logging."""
    wandb.finish()

Usage:
  wandb_logger = WandBLogger(
    project="traffic-vlm",
    experiment_name="siglip_gemma_v1",
    config=vars(training_config)
  )

  wandb_logger.watch_model(model)

  for epoch in range(num_epochs):
    # Training
    train_metrics = train_epoch(...)
    wandb_logger.log({"train/loss": train_metrics['loss']}, step=epoch)

    # Validation
    val_metrics = validate(...)
    wandb_logger.log({"val/accuracy": val_metrics['accuracy']}, step=epoch)

  wandb_logger.finish()
```

#### **22.4 Progress Tracking**

**22.4.1 Progress Bar with Rich Metrics**

```
from tqdm import tqdm

class RichProgressBar:
  """Enhanced progress bar with metrics."""

  def __init__(self, dataloader, desc="Training"):
    self.pbar = tqdm(dataloader, desc=desc)
    self.metrics_window = []
    self.window_size = 20

  def __iter__(self):
    return iter(self.pbar)

  def update(self, metrics):
    """Update progress bar with metrics."""
    self.metrics_window.append(metrics)
    if len(self.metrics_window) > self.window_size:
      self.metrics_window.pop(0)

    # Compute moving average
    avg_metrics = {}
    for key in metrics.keys():
      values = [m[key] for m in self.metrics_window if key in m]
      if values:
        avg_metrics[key] = sum(values) / len(values)

    # Update postfix
    postfix_str = {k: f"{v:.4f}" for k, v in avg_metrics.items()}
    self.pbar.set_postfix(postfix_str)

  def close(self):
    self.pbar.close()

Usage:
  pbar = RichProgressBar(train_dataloader, desc=f"Epoch {epoch+1}")

  for batch in pbar:
    loss, acc = train_step(model, batch)
    pbar.update({"loss": loss.item(), "acc": acc.item()})

  pbar.close()
```

---

### **Task 23: visualization.py - Training Visualizations**

#### **23.1 Training Curves**

**23.1.1 Loss and Accuracy Plots**

```
import matplotlib.pyplot as plt
import numpy as np

def plot_training_history(history, save_path="outputs/visualizations/training_curves.png"):
  """
  Plot training and validation curves.

  Args:
    history: Dict with keys 'train_loss', 'val_loss', 'train_accuracy', 'val_accuracy'
    save_path: Path to save plot
  """
  epochs = range(1, len(history['train_loss']) + 1)

  fig, axes = plt.subplots(1, 2, figsize=(15, 5))

  # Loss plot
  axes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)
  axes[0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)
  axes[0].set_xlabel('Epoch', fontsize=12)
  axes[0].set_ylabel('Loss', fontsize=12)
  axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
  axes[0].legend(fontsize=10)
  axes[0].grid(True, alpha=0.3)

  # Accuracy plot
  axes[1].plot(epochs, history['train_accuracy'], 'b-', label='Train Accuracy', linewidth=2)
  axes[1].plot(epochs, history['val_accuracy'], 'r-', label='Val Accuracy', linewidth=2)
  axes[1].set_xlabel('Epoch', fontsize=12)
  axes[1].set_ylabel('Accuracy', fontsize=12)
  axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
  axes[1].legend(fontsize=10)
  axes[1].grid(True, alpha=0.3)
  axes[1].set_ylim([0, 1])

  plt.tight_layout()
  plt.savefig(save_path, dpi=300, bbox_inches='tight')
  plt.close()

  print(f"Training curves saved to {save_path}")
```

**23.1.2 Learning Rate Schedule Visualization**

```
def plot_lr_schedule(history, save_path="outputs/visualizations/lr_schedule.png"):
  """Plot learning rate schedule."""
  steps = range(len(history['learning_rates']))

  plt.figure(figsize=(12, 5))
  plt.plot(steps, history['learning_rates'], 'g-', linewidth=2)
  plt.xlabel('Training Step', fontsize=12)
  plt.ylabel('Learning Rate', fontsize=12)
  plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')
  plt.grid(True, alpha=0.3)
  plt.yscale('log')  # Log scale for LR
  plt.tight_layout()
  plt.savefig(save_path, dpi=300, bbox_inches='tight')
  plt.close()

  print(f"LR schedule saved to {save_path}")
```

#### **23.2 Attention Visualization**

**23.2.1 Cross-Attention Heatmaps**

```
def visualize_cross_attention(
  image,
  command,
  attention_weights,
  save_path="outputs/visualizations/attention.png"
):
  """
  Visualize cross-attention between text tokens and image patches.

  Args:
    image: PIL Image or numpy array [H, W, 3]
    command: Text command
    attention_weights: [text_len, 196] attention matrix
    save_path: Save path
  """
  import matplotlib.pyplot as plt
  from PIL import Image
  import numpy as np

  # Convert image to numpy if needed
  if isinstance(image, Image.Image):
    image = np.array(image)

  # Tokenize command (for display)
  tokens = command.split()

  # Reshape attention to 2D grid (14x14)
  attention_2d = attention_weights.reshape(len(tokens), 14, 14)

  # Create figure
  num_tokens = min(len(tokens), 8)  # Show max 8 tokens
  fig, axes = plt.subplots(2, num_tokens, figsize=(3*num_tokens, 6))

  for i in range(num_tokens):
    # Original image
    axes[0, i].imshow(image)
    axes[0, i].set_title(f'Token: "{tokens[i]}"', fontsize=10)
    axes[0, i].axis('off')

    # Attention heatmap overlaid
    axes[1, i].imshow(image, alpha=0.6)

    # Resize attention to image size
    from scipy.ndimage import zoom
    attention_resized = zoom(attention_2d[i], (image.shape[0]/14, image.shape[1]/14))

    im = axes[1, i].imshow(attention_resized, cmap='jet', alpha=0.4)
    axes[1, i].set_title(f'Attention', fontsize=10)
    axes[1, i].axis('off')

  plt.suptitle(f'Command: "{command}"', fontsize=14, fontweight='bold')
  plt.tight_layout()
  plt.savefig(save_path, dpi=300, bbox_inches='tight')
  plt.close()

  print(f"Attention visualization saved to {save_path}")
```

**23.2.2 Attention Aggregation Across Layers**

```
def visualize_attention_across_layers(
  image,
  command,
  cross_attentions,  # List of attention matrices from all layers
  save_path="outputs/visualizations/attention_layers.png"
):
  """
  Visualize how attention evolves across decoder layers.

  Args:
    cross_attentions: List of [num_heads, text_len, 196] from each layer
  """
  # Average over heads and tokens for each layer
  layer_attentions = []
  for layer_attn in cross_attentions:
    # Average over heads and text tokens
    avg_attn = layer_attn.mean(dim=(0, 1))  # [196]
    layer_attentions.append(avg_attn.reshape(14, 14))

  # Plot
  num_layers = len(layer_attentions)
  fig, axes = plt.subplots(1, num_layers+1, figsize=(3*(num_layers+1), 3))

  # Original image
  axes[0].imshow(image)
  axes[0].set_title("Original", fontsize=10)
  axes[0].axis('off')

  # Each layer
  for i, attn in enumerate(layer_attentions):
    im = axes[i+1].imshow(attn, cmap='hot', vmin=0, vmax=attn.max())
    axes[i+1].set_title(f"Layer {i+1}", fontsize=10)
    axes[i+1].axis('off')
    plt.colorbar(im, ax=axes[i+1], fraction=0.046)

  plt.suptitle(f'Attention Evolution: "{command}"', fontsize=12, fontweight='bold')
  plt.tight_layout()
  plt.savefig(save_path, dpi=300, bbox_inches='tight')
  plt.close()
```

#### **23.3 Prediction Visualization**

**23.3.1 Correct and Incorrect Predictions Grid**

```
def visualize_predictions(
  model,
  dataloader,
  tokenizer,
  device="cuda",
  num_samples=16,
  save_path="outputs/visualizations/predictions.png"
):
  """
  Visualize model predictions on sample images.

  Shows:
    - Image
    - Command
    - True label
    - Predicted label
    - Confidence
  """
  model.eval()

  samples = []

  with torch.no_grad():
    for batch in dataloader:
      images = batch["images"].to(device)
      input_ids = batch["input_ids"].to(device)
      attention_mask = batch["attention_mask"].to(device)
      labels = batch["labels"]
      command_texts = batch["command_texts"]

      outputs = model(images, input_ids, attention_mask)
      logits = outputs["logits"]
      predictions = torch.argmax(logits, dim=-1)
      probs = torch.softmax(logits, dim=-1)

      for i in range(len(images)):
        samples.append({
          "image": images[i].cpu(),
          "command": command_texts[i],
          "true_label": ["NO", "YES"][labels[i].item()],
          "pred_label": ["NO", "YES"][predictions[i].item()],
          "confidence": probs[i, predictions[i]].item(),
          "correct": predictions[i].item() == labels[i].item()
        })

        if len(samples) >= num_samples:
          break

      if len(samples) >= num_samples:
        break

  # Plot grid
  import torchvision.transforms as T
  to_pil = T.ToPILImage()

  rows = 4
  cols = 4
  fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 5*rows))
  axes = axes.flatten()

  for i, (ax, sample) in enumerate(zip(axes, samples)):
    # Denormalize image
    image = sample["image"]
    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
    image = image * std + mean
    image = torch.clamp(image, 0, 1)

    # Display
    ax.imshow(to_pil(image))

    # Title with prediction info
    correct_symbol = "âœ“" if sample["correct"] else "âœ—"
    color = "green" if sample["correct"] else "red"

    title = f'{correct_symbol} "{sample["command"][:40]}..."\n'
    title += f'True: {sample["true_label"]}, '
    title += f'Pred: {sample["pred_label"]} ({sample["confidence"]:.2f})'

    ax.set_title(title, fontsize=9, color=color, fontweight='bold')
    ax.axis('off')

  plt.tight_layout()
  plt.savefig(save_path, dpi=200, bbox_inches='tight')
  plt.close()

  print(f"Prediction visualization saved to {save_path}")
```

---

### **Task 24: config.py - Configuration Management**

**24.1 Complete Configuration System**

**24.1.1 Master Configuration Class**

```
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional, List, Tuple
import yaml
import json

@dataclass
class VLMCompleteConfig:
  """Master configuration for Traffic VLM."""

  # ===== Experiment =====
  experiment_name: str = "traffic_vlm_v1"
  seed: int = 42
  device: str = "cuda"

  # ===== Data =====
  data_root: Path = Path("data")
  processed_data_path: Path = Path("data/processed")
  image_size: int = 224
  patch_size: int = 16
  num_train_samples: int = 4000
  num_val_samples: int = 500
  num_test_samples: int = 500

  # ===== Model Architecture =====
  # Vision Encoder
  vision_hidden_size: int = 768
  vision_num_layers: int = 6
  vision_num_heads: int = 12
  vision_intermediate_size: int = 3072

  # Projection
  projection_intermediate_size: int = 1024

  # Language Decoder
  vocab_size: int = 500
  language_hidden_size: int = 512
  decoder_num_layers: int = 4
  decoder_num_heads: int = 8
  decoder_num_kv_heads: int = 2  # GQA
  decoder_intermediate_size: int = 2048
  max_position_embeddings: int = 128

  # Classification
  num_classes: int = 2

  # Regularization
  hidden_dropout: float = 0.1
  attention_dropout: float = 0.0

  # RoPE
  rope_theta: float = 10000.0

  # Normalization
  rms_norm_eps: float = 1e-6
  layer_norm_eps: float = 1e-6

  # Weight tying
  tie_word_embeddings: bool = True

  # ===== Training =====
  num_epochs: int = 20
  batch_size: int = 4
  gradient_accumulation_steps: int = 4

  # Optimization
  learning_rate: float = 1e-4
  vision_lr: float = 1e-5
  projection_lr: float = 5e-4
  decoder_lr: float = 1e-4
  classifier_lr: float = 5e-4
  use_differential_lr: bool = False

  weight_decay: float = 0.01
  max_grad_norm: float = 1.0

  # Scheduler
  warmup_ratio: float = 0.1
  min_lr_ratio: float = 0.1
  scheduler_type: str = "cosine_warmup"

  # Loss
  label_smoothing: float = 0.0
  class_weights: Optional[List[float]] = None

  # Mixed precision
  use_amp: bool = True

  # Early stopping
  early_stopping: bool = True
  patience: int = 5

  # ===== Data Loading =====
  num_workers: int = 2
  pin_memory: bool = True
  prefetch_factor: int = 2
  persistent_workers: bool = True

  # ===== Paths =====
  checkpoint_dir: Path = Path("checkpoints")
  log_dir: Path = Path("logs")
  output_dir: Path = Path("outputs")
  visualization_dir: Path = Path("outputs/visualizations")

  # ===== Logging =====
  log_every: int = 100
  eval_every: int = 1
  save_every: int = 5
  use_tensorboard: bool = True
  use_wandb: bool = False
  wandb_project: Optional[str] = None

  # ===== Evaluation =====
  eval_batch_size: int = 8
  compute_attention_maps: bool = True
  save_predictions: bool = True

  def __post_init__(self):
    """Create directories and validate config."""
    # Create directories
    for path in [self.checkpoint_dir, self.log_dir,
                 self.output_dir, self.visualization_dir]:
      path.mkdir(parents=True, exist_ok=True)

    # Validate
    assert self.language_hidden_size % self.decoder_num_heads == 0, \
      "language_hidden_size must be divisible by decoder_num_heads"
    assert self.decoder_num_heads % self.decoder_num_kv_heads == 0, \
      "decoder_num_heads must be divisible by decoder_num_kv_heads"
    assert self.vision_hidden_size % self.vision_num_heads == 0, \
      "vision_hidden_size must be divisible by vision_num_heads"

  def save(self, path: Path):
    """Save config to YAML file."""
    with open(path, 'w') as f:
      yaml.dump(self.to_dict(), f, default_flow_style=False)
    print(f"Config saved to {path}")

  @classmethod
  def load(cls, path: Path):
    """Load config from YAML file."""
    with open(path, 'r') as f:
      config_dict = yaml.safe_load(f)
    return cls(**config_dict)

  def to_dict(self):
    """Convert to dictionary."""
    return {k: str(v) if isinstance(v, Path) else v
            for k, v in self.__dict__.items()}

  def print_summary(self):
    """Print configuration summary."""
    print("="*80)
    print("CONFIGURATION SUMMARY")
    print("="*80)

    sections = {
      "Experiment": ["experiment_name", "seed", "device"],
      "Data": ["num_train_samples", "num_val_samples", "image_size"],
      "Model": ["vision_hidden_size", "language_hidden_size",
                "decoder_num_layers", "vocab_size"],
      "Training": ["num_epochs", "batch_size", "learning_rate",
                   "gradient_accumulation_steps"],
      "Optimization": ["weight_decay", "max_grad_norm", "use_amp"]
    }

    for section, keys in sections.items():
      print(f"\n{section}:")
      for key in keys:
        if hasattr(self, key):
          print(f"  {key}: {getattr(self, key)}")

    print("\n" + "="*80)
```

**24.1.2 Config Presets**

```
def get_small_config():
  """Small model for quick experiments."""
  return VLMCompleteConfig(
    experiment_name="traffic_vlm_small",
    vision_hidden_size=512,
    vision_num_layers=4,
    language_hidden_size=256,
    decoder_num_layers=2,
    decoder_num_heads=4,
    decoder_num_kv_heads=2,
    batch_size=8
  )

def get_medium_config():
  """Medium model (default)."""
  return VLMCompleteConfig(
    experiment_name="traffic_vlm_medium"
  )

def get_large_config():
  """Large model (requires more VRAM)."""
  return VLMCompleteConfig(
    experiment_name="traffic_vlm_large",
    vision_hidden_size=1024,
    vision_num_layers=8,
    language_hidden_size=768,
    decoder_num_layers=6,
    decoder_num_heads=12,
    batch_size=2
  )

Usage:
  # Load preset
  config = get_medium_config()

  # Override specific values
  config.num_epochs = 30
  config.learning_rate = 5e-5

  # Save
  config.save(Path("configs/my_experiment.yaml"))

  # Load later
  config = VLMCompleteConfig.load(Path("configs/my_experiment.yaml"))
```

---

### **Task 25: main.py - Complete Training Script**

**25.1 Main Training Script**

```python
#!/usr/bin/env python3
"""
Main training script for Traffic VLM.

Usage:
  python main.py --config configs/default.yaml
  python main.py --experiment_name my_experiment --num_epochs 30
"""

import argparse
import torch
import torch.nn as nn
from pathlib import Path
import random
import numpy as np

# Import all components
from config import VLMCompleteConfig, get_medium_config
from vlm_model import TrafficVLM
from data_loader import TrafficVLMDataset, collate_fn
from tokenizer import TrafficCommandTokenizer
from optimizer import create_optimizer_and_scheduler
from train import train
from evaluation import validate, compute_classification_metrics, print_metrics
from logger import TrainingLogger, TensorBoardLogger
from visualization import plot_training_history, plot_lr_schedule

def set_seed(seed):
  """Set random seed for reproducibility."""
  random.seed(seed)
  np.random.seed(seed)
  torch.manual_seed(seed)
  torch.cuda.manual_seed_all(seed)
  torch.backends.cudnn.deterministic = True
  torch.backends.cudnn.benchmark = False
  print(f"Random seed set to {seed}")

def parse_args():
  """Parse command line arguments."""
  parser = argparse.ArgumentParser(description="Train Traffic VLM")

  # Config file
  parser.add_argument("--config", type=str, help="Path to config YAML file")

  # Experiment
  parser.add_argument("--experiment_name", type=str, help="Experiment name")
  parser.add_argument("--seed", type=int, help="Random seed")

  # Training
  parser.add_argument("--num_epochs", type=int, help="Number of epochs")
  parser.add_argument("--batch_size", type=int, help="Batch size")
  parser.add_argument("--learning_rate", type=float, help="Learning rate")

  # Resume
  parser.add_argument("--resume", type=str, help="Path to checkpoint to resume from")

  # Evaluation only
  parser.add_argument("--eval_only", action="store_true", help="Only evaluate, don't train")
  parser.add_argument("--checkpoint", type=str, help="Checkpoint path for evaluation")

  return parser.parse_args()

def main():
  """Main training pipeline."""

  # Parse arguments
  args = parse_args()

  # Load config
  if args.config:
    config = VLMCompleteConfig.load(Path(args.config))
  else:
    config = get_medium_config()

  # Override config with CLI args
  if args.experiment_name:
    config.experiment_name = args.experiment_name
  if args.seed:
    config.seed = args.seed
  if args.num_epochs:
    config.num_epochs = args.num_epochs
  if args.batch_size:
    config.batch_size = args.batch_size
  if args.learning_rate:
    config.learning_rate = args.learning_rate

  # Print config
  config.print_summary()

  # Save config
  config.save(config.log_dir / f"{config.experiment_name}_config.yaml")

  # Set seed
  set_seed(config.seed)

  # Setup device
  device = torch.device(config.device if torch.cuda.is_available() else "cpu")
  print(f"\nUsing device: {device}")
  if device.type == "cuda":
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

  # ===== Load Tokenizer =====
  print("\n" + "="*80)
  print("Loading Tokenizer...")
  print("="*80)

  tokenizer = TrafficCommandTokenizer()
  tokenizer.load_vocab(config.processed_data_path / "vocab.json")
  print(f"Vocabulary size: {tokenizer.vocab_size}")

  # ===== Create Datasets =====
  print("\n" + "="*80)
  print("Creating Datasets...")
  print("="*80)

  train_dataset = TrafficVLMDataset(
    split="train",
    config=config,
    tokenizer=tokenizer,
    transform=None  # Transforms handled in dataset
  )

  val_dataset = TrafficVLMDataset(
    split="val",
    config=config,
    tokenizer=tokenizer,
    transform=None
  )

  print(f"Train samples: {len(train_dataset)}")
  print(f"Val samples: {len(val_dataset)}")

  # ===== Create DataLoaders =====
  from torch.utils.data import DataLoader

  train_dataloader = DataLoader(
    train_dataset,
    batch_size=config.batch_size,
    shuffle=True,
    num_workers=config.num_workers,
    pin_memory=config.pin_memory,
    collate_fn=collate_fn,
    persistent_workers=config.persistent_workers if config.num_workers > 0 else False,
    prefetch_factor=config.prefetch_factor if config.num_workers > 0 else None
  )

  val_dataloader = DataLoader(
    val_dataset,
    batch_size=config.eval_batch_size,
    shuffle=False,
    num_workers=config.num_workers,
    pin_memory=config.pin_memory,
    collate_fn=collate_fn,
    persistent_workers=config.persistent_workers if config.num_workers > 0 else False
  )

  print(f"Train batches: {len(train_dataloader)}")
  print(f"Val batches: {len(val_dataloader)}")

  # ===== Create Model =====
  print("\n" + "="*80)
  print("Creating Model...")
  print("="*80)

  model = TrafficVLM(config)
  model.to(device)

  total_params = sum(p.numel() for p in model.parameters())
  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

  print(f"Total parameters: {total_params:,}")
  print(f"Trainable parameters: {trainable_params:,}")
  print(f"Model size: {total_params * 4 / 1e6:.2f} MB (FP32)")

  # ===== Evaluation Only =====
  if args.eval_only:
    print("\n" + "="*80)
    print("Evaluation Mode")
    print("="*80)

    if args.checkpoint:
      checkpoint = torch.load(args.checkpoint, map_location=device)
      model.load_state_dict(checkpoint["model_state_dict"])
      print(f"Loaded checkpoint from {args.checkpoint}")

    model.eval()
    val_metrics = validate(model, val_dataloader, device)
    print_metrics(val_metrics, title="Validation Metrics")

    return

  # ===== Create Optimizer & Scheduler =====
  print("\n" + "="*80)
  print("Creating Optimizer & Scheduler...")
  print("="*80)

  optimizer, scheduler = create_optimizer_and_scheduler(
    model,
    train_dataloader,
    config
  )

  # ===== Setup Logging =====
  print("\n" + "="*80)
  print("Setup Logging...")
  print("="*80)

  logger = TrainingLogger(
    log_dir=config.log_dir,
    experiment_name=config.experiment_name
  )
  logger.log_hyperparameters(config)
  logger.log_model_summary(model)

  tb_logger = None
  if config.use_tensorboard:
    tb_logger = TensorBoardLogger(
      log_dir="runs",
      experiment_name=config.experiment_name
    )
    tb_logger.watch_model(model)

  # ===== Resume from Checkpoint =====
  start_epoch = 0
  if args.resume:
    print(f"\nResuming from checkpoint: {args.resume}")
    checkpoint = torch.load(args.resume, map_location=device)
    model.load_state_dict(checkpoint["model_state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
    if scheduler and "scheduler_state_dict" in checkpoint:
      scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
    start_epoch = checkpoint["epoch"] + 1
    print(f"Resuming from epoch {start_epoch}")

  # ===== Training Loop =====
  print("\n" + "="*80)
  print("Starting Training...")
  print("="*80)

  history = train(
    model=model,
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    optimizer=optimizer,
    scheduler=scheduler,
    config=config,
    device=device,
    logger=logger,
    tb_logger=tb_logger,
    start_epoch=start_epoch
  )

  # ===== Final Evaluation =====
  print("\n" + "="*80)
  print("Final Evaluation...")
  print("="*80)

  # Load best model
  best_checkpoint_path = config.checkpoint_dir / "best_model.pt"
  if best_checkpoint_path.exists():
    checkpoint = torch.load(best_checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint["model_state_dict"])
    print(f"Loaded best model from {best_checkpoint_path}")

  # Validate
  final_metrics = validate(model, val_dataloader, device)
  print_metrics(final_metrics, title="Final Validation Metrics")

  # ===== Save Results =====
  logger.save_metrics()

  # Plot training curves
  plot_training_history(
    history,
    save_path=config.visualization_dir / "training_curves.png"
  )

  plot_lr_schedule(
    history,
    save_path=config.visualization_dir / "lr_schedule.png"
  )

  # Close loggers
  if tb_logger:
    tb_logger.close()

  print("\n" + "="*80)
  print("Training Complete!")
  print("="*80)
  print(f"Best validation accuracy: {max(history['val_accuracy']):.4f}")
  print(f"Checkpoints saved to: {config.checkpoint_dir}")
  print(f"Logs saved to: {config.log_dir}")
  print(f"Visualizations saved to: {config.visualization_dir}")

if __name__ == "__main__":
  main()
```

**25.2 Quick Start Scripts**

**25.2.1 train.sh**

```bash
#!/bin/bash

# Quick training script

# Set environment
export CUDA_VISIBLE_DEVICES=0

# Run training
python main.py \
  --experiment_name "traffic_vlm_baseline" \
  --num_epochs 20 \
  --batch_size 4 \
  --learning_rate 1e-4

echo "Training complete!"
```

**25.2.2 evaluate.sh**

```bash
#!/bin/bash

# Evaluation script

python main.py \
  --eval_only \
  --checkpoint checkpoints/best_model.pt \
  --experiment_name "evaluation"
```

---

## Summary of Phase 6 (Tasks 18-25)

### **Completed Training Pipeline:**

1. **Task 18: Loss Functions** âœ“

   - Cross-entropy classification loss
   - Label smoothing
   - Class weights for imbalanced data
   - Optional auxiliary losses

2. **Task 19: Optimizer** âœ“

   - AdamW optimizer
   - Differential learning rates
   - Cosine warmup scheduler
   - Gradient clipping
   - Mixed precision training

3. **Task 20: Training Loop** âœ“

   - Epoch-based training
   - Gradient accumulation
   - Checkpointing
   - Early stopping
   - Resume training

4. **Task 21: Evaluation** âœ“

   - Comprehensive metrics (accuracy, F1, precision, recall)
   - Confusion matrix
   - Per-command-type analysis
   - Error analysis
   - Calibration analysis

5. **Task 22: Logging** âœ“

   - File logging
   - TensorBoard integration
   - Weights & Biases support
   - Progress bars
   - Metric tracking

6. **Task 23: Visualization** âœ“

   - Training curves
   - LR schedule plots
   - Attention heatmaps
   - Prediction grids
   - Confusion matrices

7. **Task 24: Configuration** âœ“

   - Complete config system
   - YAML serialization
   - Config presets
   - CLI overrides

8. **Task 25: Main Script** âœ“
   - End-to-end training pipeline
   - Evaluation mode
   - Resume capability
   - Shell scripts

### **Ready to Train!**

Complete pipeline from data to trained model:

1. Generate data (Tasks 1-5)
2. Create model (Tasks 6-17)
3. Train model (Tasks 18-25)
4. Evaluate & visualize results

**Estimated Training Time (A3000):**

- Small model (4M params): 1-2 hours
- Medium model (44M params): 4-6 hours
- Large model (100M params): 10-12 hours

All 25 tasks completed! Ready to start training your Traffic Scene VLM! ðŸš€
